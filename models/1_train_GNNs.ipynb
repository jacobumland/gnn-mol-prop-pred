{"cells":[{"cell_type":"markdown","metadata":{},"source":["*this notebook was run in Google Colab*"]},{"cell_type":"markdown","metadata":{"id":"PsC5GZYb3b-R"},"source":["# setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MCuvfiwLn9ez","outputId":"25c8c7e8-6b30-4502-972c-dc0a75c7017f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Colab: mounting Google drive on  /content/gdrive\n"]}],"source":["# mount notebook\n","from google.colab import drive\n","\n","mount='/content/gdrive'\n","print(\"Colab: mounting Google drive on \", mount)\n","\n","drive.mount(mount)\n","\n","# switch to the directory on the Google Drive that you want to use\n","import os\n","drive_root = mount + \"/My Drive/Colab Notebooks/thesis_training_models\"\n","  \n","# create drive_root if it doesn't exist\n","create_drive_root = True\n","if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","\n","# change to the directory\n","print(\"\\nColab: Changing directory to \", drive_root)\n","%cd $drive_root"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dllo2coP3AQh"},"outputs":[],"source":["# check computational resources: GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cg2PWhQZ3KUt"},"outputs":[],"source":["# check computational resources: RAM\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S66nZnfo3QSi"},"outputs":[],"source":["# installations\n","\n","# ogb\n","! pip install ogb\n","\n","# dgl\n","! pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html\n","\n","# mydgllife\n","! pip install git+https://github.com/jacobumland/my-dgl-lifesci.git#subdirectory=python\n","\n","# rdkit\n","! pip install rdkit "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FhtS-X7Q3ZtE"},"outputs":[],"source":["# imports\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import copy\n","import dgl\n","import time\n","from ogb.graphproppred import DglGraphPropPredDataset, collate_dgl\n","from torch.utils.data import DataLoader\n","from ogb.graphproppred import Evaluator\n","from mydgllife.model.model_zoo import * # import all models from model zoo\n","from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n","from dgl import backend as F\n","from os.path import exists as file_exists\n","import pickle\n","import numpy as np\n","from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc\n","import pandas as pd\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0nAzaSOxARF"},"outputs":[],"source":["# set device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"markdown","metadata":{"id":"G-Pou2-ss7b0"},"source":["# helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dh2Oejo6tH95"},"outputs":[],"source":["def train_epoch(model, device, data_loader, opt, loss_fn):\n","    model.train()\n","    train_loss = []\n","    for g, labels in data_loader:\n","        g = g.to(device)\n","        labels = labels.to(torch.float32).to(device)\n","        \n","        if model_name in [\"gcn\", \"gat\", \"nfp\"]:\n","            logits = model(g, g.ndata['feat'].float())  # only taking into account node features\n","        elif model_name in [\"attfp\", \"pagt\", \"weave\", \"mpnn\"]:\n","            logits =  model(g, g.ndata['feat'].float(), g.edata['feat'].float()) # taking into account node and edge features\n","\n","        loss = loss_fn(logits, labels)\n","        train_loss.append(loss.item())\n","        \n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","    return sum(train_loss) / len(train_loss) # return average loss from epoch\n","\n","def eval_epoch(model, device, data_loader, evaluator):\n","    model.eval()\n","    y_true, y_pred = [], []\n","\n","    for g, labels in data_loader:\n","        g = g.to(device)\n","        if model_name in [\"gcn\", \"gat\", \"nfp\"]:\n","            logits = model(g, g.ndata['feat'].float())  # only taking into account node features\n","        elif model_name in [\"attfp\", \"pagt\", \"weave\", \"mpnn\"]:\n","            logits =  model(g, g.ndata['feat'].float(), g.edata['feat'].float()) # taking into account node and edge features        \n","        y_true.append(labels.detach().cpu())\n","        y_pred.append(logits.detach().cpu())\n","    \n","    y_true = torch.cat(y_true, dim=0).numpy()\n","    y_pred = torch.cat(y_pred, dim=0).numpy()\n","\n","    return evaluator.eval({\n","        'y_true': y_true,\n","        'y_pred': y_pred\n","    })['rocauc']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YfUnydbV3mI2"},"outputs":[],"source":["def eval_epoch_statistical(model, device, data_loader, evaluator):\n","    model.eval()\n","    y_true, y_pro = [], []\n","\n","    for g, labels in data_loader:\n","        g = g.to(device)\n","        if model_name in [\"gcn\", \"gat\", \"nfp\"]:\n","            logits = model(g, g.ndata['feat'].float())  # only taking into account node features\n","        elif model_name in [\"attfp\", \"pagt\", \"weave\", \"mpnn\"]:\n","            logits =  model(g, g.ndata['feat'].float(), g.edata['feat'].float()) # taking into account node and edge features        \n","        y_true.append(labels.detach().cpu())\n","        y_pro.append(logits.detach().cpu())\n","    \n","    y_true = torch.cat(y_true, dim=0).numpy()\n","    y_pro = torch.cat(y_pro, dim=0).numpy()\n","    y_pred = np.argmax(y_pro, axis=1)\n","\n","    tn, fp, fn, tp, se, sp, acc, mcc, auc_prc, auc_roc = statistical(y_true, y_pred, y_pro)\n","\n","    return tn, fp, fn, tp, se, sp, acc, mcc, auc_prc, auc_roc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZQVUwrQtIGo"},"outputs":[],"source":["# metrics\n","def statistical(y_true, y_pred, y_pro):\n","    c_mat = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = list(c_mat.flatten())\n","    se = tp / (tp + fn)\n","    sp = tn / (tn + fp)\n","    acc = (tp + tn) / (tn + fp + fn + tp)\n","    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n","    auc_prc = auc(precision_recall_curve(y_true, y_pro, pos_label=1)[1],\n","                  precision_recall_curve(y_true, y_pro, pos_label=1)[0])\n","    auc_roc = roc_auc_score(y_true, y_pro)\n","    return tn, fp, fn, tp, se, sp, acc, mcc, auc_prc, auc_roc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY3tk3irwhYd"},"outputs":[],"source":["# hyperparams opt functions\n","def hyper_opt(hyper_space):\n","    \n","    # get the model instance\n","    if model_name == \"attfp\":\n","        model = AttentiveFPPredictor(node_feat_size = node_feat_dim,\n","                                     edge_feat_size = edge_feat_dim,\n","                                     num_layers = hyper_space[\"num_layers\"],\n","                                     num_timesteps = hyper_space[\"num_timesteps\"],\n","                                     graph_feat_size = hyper_space[\"graph_feat_size\"],\n","                                     dropout = hyper_space[\"dropout\"])\n","        \n","        \n","\n","    elif model_name == \"gat\":\n","        model = GATPredictor(in_feats = node_feat_dim,\n","                             hidden_feats = hyper_space[\"hidden_feats\"],\n","                             num_heads = hyper_space[\"num_heads\"],\n","                             predictor_hidden_feats = hyper_space[\"predictor_hidden_feats\"],\n","                             predictor_dropout = hyper_space[\"predictor_dropout\"])\n","\n","    elif model_name == \"gcn\":\n","        model = GCNPredictor(in_feats = node_feat_dim,\n","                             hidden_feats = hyper_space[\"hidden_feats\"],\n","                             dropout = hyper_space[\"dropout\"],\n","                             predictor_hidden_feats = hyper_space[\"predictor_hidden_feats\"],\n","                             predictor_dropout = hyper_space[\"predictor_dropout\"])\n","\n","    elif model_name == \"mpnn\":\n","        model = MPNNPredictor(node_in_feats = node_feat_dim,\n","                              edge_in_feats = edge_feat_dim,\n","                              node_out_feats = hyper_space[\"node_out_feats\"],\n","                              edge_hidden_feats = hyper_space[\"edge_hidden_feats\"],\n","                              num_step_message_passing = hyper_space[\"num_step_message_passing\"],\n","                              num_step_set2set = hyper_space[\"num_step_set2set\"],\n","                              num_layer_set2set = hyper_space[\"num_layer_set2set\"])\n","\n","    elif model_name == \"weave\":\n","        model = WeavePredictor(node_in_feats = node_feat_dim,\n","                               edge_in_feats = edge_feat_dim,\n","                               gnn_hidden_feats = hyper_space[\"gnn_hidden_feats\"],\n","                               graph_feats =  hyper_space[\"graph_feats\"],\n","                               num_gnn_layers = hyper_space[\"num_gnn_layers\"]\n","                               )\n","\n","    elif model_name == \"nfp\":\n","        model = NFPredictor(in_feats = node_feat_dim, \n","                            hidden_feats= hyper_space[\"hidden_feats\"],\n","                            max_degree = hyper_space[\"max_degree\"],\n","                            dropout = hyper_space[\"dropout\"],\n","                            predictor_hidden_size = hyper_space[\"predictor_hidden_size\"],\n","                            predictor_dropout = hyper_space[\"predictor_dropout\"]\n","                            )\n","\n","    elif model_name == \"pagt\":\n","        model = PAGTNPredictor(node_in_feats =node_feat_dim,\n","                               edge_feats =edge_feat_dim,\n","                               node_out_feats = hyper_space[\"node_out_feats\"],\n","                               node_hid_feats = hyper_space[\"node_hid_feats\"],\n","                               depth=hyper_space[\"depth\"],\n","                               nheads=hyper_space[\"nheads\"],\n","                               dropout=hyper_space[\"dropout\"]\n","                               )\n","\n","    # loss function\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device)) # for classification\n","\n","    # optimizer\n","    opt = optim.Adam(model.parameters(), lr=hyper_space[\"lr\"], weight_decay=hyper_space[\"l2\"])\n","\n","    # load model \n","    model = model.to(device)\n","\n","    # training\n","    best_auc = 0\n","    best_model = copy.deepcopy(model)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    \n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model, device, train_loader, opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","\n","        # print(f'epoch {j} | Train Loss: {train_loss:.4f} | Train Auc: {train_auc:.4f} | Valid Auc: {valid_auc:.4f}')\n","\n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","\n","        #if valid_auc > best_auc:\n","        #    best_auc = valid_auc\n","        #    best_model = copy.deepcopy(model)\n","\n","    # test auc  \n","    test_auc = eval_epoch(best_model, device, test_loader, evaluator)\n","\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiKploxtuUcS"},"outputs":[],"source":["\"\"\"# patience when no increase in X\n","# j is number of epoch\n","# we need another indicator of when the last update was made (the last increase)\n","epoks = 3\n","increase_idx = 0\n","patienc = 2\n","\n","for j in range(epoks):\n","    print(f\"j {j}\")\n","    # checks if there was an update\n","    if float(np.random.rand(1,1)) > float(np.random.rand(1,1)):\n","        increase_idx = j\n","        # save model\n","    # checks if the last update is X (patience) ago\n","    if j - increase_idx == patienc:\n","        break\n","    print(f\"increase_idx {increase_idx}\")\n","\n","# increase_idx = \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EN6pDYEjvPNU"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w0idN2_CvMQJ"},"outputs":[],"source":["float(np.random.rand(1,1)) > float(np.random.rand(1,1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjj1qklG64u4"},"outputs":[],"source":["# calculate positive weight\n","def get_pos_weight(data):\n","    num_pos = F.sum(data.labels, dim=0)\n","    num_indices = F.tensor(len(data.labels))\n","    return (num_indices - num_pos) / num_pos"]},{"cell_type":"markdown","metadata":{"id":"ZPm0WWxGtDCP"},"source":["# data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0PGdwcvFw7sQ"},"outputs":[],"source":["# data & evaluator\n","dataset = DglGraphPropPredDataset(name=\"ogbg-molhiv\")\n","evaluator = Evaluator(name=\"ogbg-molhiv\")\n","g, _ = dataset[0]\n","node_feat_dim = g.ndata['feat'].size()[-1]\n","edge_feat_dim = g.edata['feat'].size()[-1]\n","\n","batch_size = 128\n","\n","split_idx = dataset.get_idx_split()\n","train_loader = DataLoader(dataset[split_idx[\"train\"]],\n","                            batch_size=batch_size,\n","                            shuffle=True,\n","                            collate_fn=collate_dgl)\n","valid_loader = DataLoader(dataset[split_idx[\"valid\"]],\n","                            batch_size=batch_size,\n","                            shuffle=False,\n","                            collate_fn=collate_dgl)\n","test_loader = DataLoader(dataset[split_idx[\"test\"]],\n","                            batch_size=batch_size,\n","                            shuffle=False,\n","                            collate_fn=collate_dgl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYnZMVw_6eqg"},"outputs":[],"source":["# calculare positive weight\n","pos_weight = get_pos_weight(dataset)"]},{"cell_type":"markdown","metadata":{"id":"YYybK018s9PS"},"source":["# experimentation overview"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5EOH-8Fu5P9"},"outputs":[],"source":["overview_df_filename = \"experimentation_overview\"\n","if not file_exists(overview_df_filename):\n","    print(\"no experiments conducted yet, please run the models\")\n","else:\n","    overview_df = pd.read_parquet(overview_df_filename)\n","    display(overview_df)"]},{"cell_type":"markdown","metadata":{"id":"e7fVIH9auwgw"},"source":["# GNNs"]},{"cell_type":"markdown","metadata":{"id":"5RdZ5-n84NzQ"},"source":["which GNNs to include?\n","- AttentiveFPPredictor\n","- GATPredictor\n","- GCNPredictor\n","- MPNNPredictor\n","- WeavePredictor\n","- NFPredictor\n","- PAGTNPredictor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IE-3i-ytvy39"},"outputs":[],"source":["# hyper space across models\n","hspace_gnns = {'attfp': dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]),\n","                                     lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]), # default: default: 1e-3\n","                                     num_layers=hp.choice('num_layers', [2, 3, 4, 5, 6]), # default: 2\n","                                     num_timesteps=hp.choice('num_timesteps', [1, 2, 3, 4, 5]), # default: 2\n","                                     graph_feat_size=hp.choice('graph_feat_size', [50, 100, 200, 300]), # default: 200\n","                                     dropout=hp.choice('dropout', [0, 0.1, 0.3, 0.5])\n","                                     ),\n","                 \n","                 'gat': dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]),\n","                             lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]),\n","                             hidden_feats=hp.choice('hidden_feats', [[64, 64], [128, 128], [256, 256], [128, 64], [256, 128]]),\n","                             num_heads=hp.choice('num_heads', [[2, 2], [3, 3], [4, 4], [4, 3], [3, 2]]),\n","                             predictor_hidden_feats=hp.choice('predictor_hidden_feats', [128, 64, 256]),\n","                             predictor_dropout=hp.choice('predictor_dropout', [0, 0.1, 0.2, 0.3, 0.4])\n","                             ),              \n","\n","                 'gcn': dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]), \n","                             lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]), # default: default: 1e-3\n","                             hidden_feats=hp.choice('hidden_feats', [[64, 64], [128, 128], [256, 256], [128, 64], [256, 128]]), # default: 2 GCN layers [64, 64]\n","                             dropout=hp.choice('dropout', [[0, 0], [0.1, 0.1], [0.2, 0.2], [0.3, 0.3], [0.4, 0.4], [0.5, 0.5]]), # default: 0\n","                             predictor_hidden_feats=hp.choice('predictor_hidden_feats', [128, 64, 256]), # default 128\n","                             predictor_dropout=hp.choice('predictor_dropout', [0, 0.1, 0.2, 0.3, 0.4, 0.5])\n","                             ),\n","                 \n","                 'mpnn': dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]), \n","                              lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]),\n","                              node_out_feats=hp.choice('node_out_feats', [128, 64, 32, 16]),\n","                              edge_hidden_feats=hp.choice('edge_hidden_feats', [128, 64, 32, 16]),\n","                              num_step_message_passing=hp.choice('num_step_message_passing', [2, 4, 6, 8]),\n","                              num_step_set2set= hp.choice('num_step_set2set', [2, 4, 6, 8])\n","                              ),\n","\n","                 'weave':dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]), \n","                              lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]),\n","                              gnn_hidden_feats = hp.choice('gnn_hidden_feats', [128, 64, 50, 32]),\n","                              num_gnn_layers = hp.choice('num_gnn_layers', [2, 3]),\n","                              graph_feats = hp.choice('graph_feats', [128, 64, 50, 32])\n","                              ),\n","\n","                 'nfp': dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]),\n","                             lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]),\n","                             hidden_feats=hp.choice('hidden_feats', [[64, 64], [128, 128], [256, 256], [128, 64], [256, 128]]),\n","                             dropout = hp.choice('dropout', [[0, 0], [0.1, 0.1], [0.2, 0.2], [0.3, 0.3], [0.4, 0.4], [0.5, 0.5]]),\n","                             predictor_hidden_size = hp.choice('predictor_hidden_size', [256, 128, 64, 32]),\n","                             predictor_dropout = hp.choice('predictor_dropout', [0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n","                             ),\n","\n","                 'pagt': dict(l2=hp.choice('l2', [0, 10 ** -8, 10 ** -6, 10 ** -4]),\n","                               lr=hp.choice('lr', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]),\n","                               node_hid_feats =hp.choice('node_hid_feats', [32, 64, 128]),\n","                               node_out_feats=hp.choice('node_out_feats', [128, 64, 32, 16]),\n","                               depth=hp.choice('depth', [3, 5, 7]),\n","                               nheads=hp.choice('nheads', [1, 2, 3, 4]),\n","                               dropout=hp.choice('dropout', [0, 0.1, 0.2, 0.3, 0.4, 0.5]),\n","                               )\n","                 }\n","\n","# lists for hp.choice for constructing models after optimization\n","# attfp\n","attfp_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","attfp_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","attfp_num_layers_ls=[2, 3, 4, 5, 6]\n","attfp_num_timesteps_ls=[1, 2, 3, 4, 5]\n","attfp_graph_feat_size_ls=[50, 100, 200, 300]\n","attfp_dropout_ls=[0, 0.1, 0.3, 0.5]\n","\n","# gat\n","gat_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","gat_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","gat_hidden_feats_ls=[[64, 64], [128, 128], [256, 256], [128, 64], [256, 128]]\n","gat_num_heads_ls=[[2, 2], [3, 3], [4, 4], [4, 3], [3, 2]]\n","gat_predictor_hidden_feats_ls=[128, 64, 256]\n","gat_predictor_dropout_ls=[0, 0.1, 0.2, 0.3, 0.4]\n","\n","\n","# gcn\n","gcn_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","gcn_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","gcn_hidden_feats_ls=[[64, 64], [128, 128], [256, 256], [128, 64], [256, 128]]\n","gcn_dropout_ls=[[0, 0], [0.1, 0.1], [0.2, 0.2], [0.3, 0.3], [0.4, 0.4], [0.5, 0.5]]\n","gcn_predictor_hidden_feats_ls=[128, 64, 256]\n","gcn_predictor_dropout_ls=[0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","# mpnn\n","mpnn_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","mpnn_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","mpnn_node_out_feats_ls=[128, 64, 32, 16]\n","mpnn_edge_hidden_feats_ls=[128, 64, 32, 16]\n","mpnn_num_step_message_passing_ls=[2, 4, 6, 8]\n","mpnn_num_step_set2set_ls=[2, 3, 4]\n","\n","# weave\n","weave_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","weave_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","weave_gnn_hidden_feats_ls=[128, 64, 50, 32]\n","weave_num_gnn_layers_ls=[2, 3]\n","weave_graph_feats_ls=[128, 64, 50, 32]\n","\n","# nfp\n","nfp_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","nfp_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","nfp_hidden_feats_ls=[[64, 64], [128, 128], [256, 256], [128, 64], [256, 128]]\n","nfp_dropout_ls=[[0, 0], [0.1, 0.1], [0.2, 0.2], [0.3, 0.3], [0.4, 0.4], [0.5, 0.5]]\n","nfp_predictor_hidden_size_ls=[256, 128, 64, 32]\n","nfp_predictor_dropout_ls=[0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","\n","# pagt\n","pagt_l2_ls=[0, 10 ** -8, 10 ** -6, 10 ** -4]\n","pagt_lr_ls=[10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","pagt_node_hid_feats_ls=[32, 64, 128]\n","pagt_node_out_feats_ls=[128, 64, 32, 16]\n","pagt_depth_ls=[3, 5, 7]\n","pagt_nheads_ls=[1, 2, 3, 4]\n","pagt_dropout_ls=[0, 0.1, 0.2, 0.3, 0.4, 0.5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4sQY4fxMxix2"},"outputs":[],"source":["# hyperparameter optimization parameters\n","epochs = 200\n","patience = 50\n","OPT_ITERS = 30\n","repetitions = 5 "]},{"cell_type":"markdown","metadata":{"id":"0HZc1DSDwr-D"},"source":["## GCN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":9136964,"status":"error","timestamp":1659085688041,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"l-MpsloL7kID","outputId":"b08b729c-979e-422b-e20c-bd412b31acc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","early stopping at epoch 142 with patience set to 50\n","early stopping at epoch 204 with patience set to 50\n","early stopping at epoch 87 with patience set to 50\n","early stopping at epoch 97 with patience set to 50\n","early stopping at epoch 103 with patience set to 50\n","early stopping at epoch 118 with patience set to 50\n","early stopping at epoch 176 with patience set to 50\n","early stopping at epoch 112 with patience set to 50\n","early stopping at epoch 91 with patience set to 50\n","early stopping at epoch 95 with patience set to 50\n","early stopping at epoch 98 with patience set to 50\n","early stopping at epoch 95 with patience set to 50\n","early stopping at epoch 160 with patience set to 50\n","early stopping at epoch 149 with patience set to 50\n","early stopping at epoch 153 with patience set to 50\n","early stopping at epoch 116 with patience set to 50\n","early stopping at epoch 98 with patience set to 50\n","early stopping at epoch 56 with patience set to 50\n","early stopping at epoch 105 with patience set to 50\n","early stopping at epoch 216 with patience set to 50\n","early stopping at epoch 197 with patience set to 50\n","early stopping at epoch 160 with patience set to 50\n","early stopping at epoch 174 with patience set to 50\n","early stopping at epoch 105 with patience set to 50\n","early stopping at epoch 131 with patience set to 50\n","early stopping at epoch 146 with patience set to 50\n","early stopping at epoch 175 with patience set to 50\n","early stopping at epoch 95 with patience set to 50\n","early stopping at epoch 201 with patience set to 50\n","early stopping at epoch 89 with patience set to 50\n","early stopping at epoch 151 with patience set to 50\n","early stopping at epoch 77 with patience set to 50\n","early stopping at epoch 104 with patience set to 50\n","early stopping at epoch 155 with patience set to 50\n","early stopping at epoch 158 with patience set to 50\n","early stopping at epoch 174 with patience set to 50\n","early stopping at epoch 115 with patience set to 50\n","early stopping at epoch 133 with patience set to 50\n","early stopping at epoch 145 with patience set to 50\n","early stopping at epoch 182 with patience set to 50\n","early stopping at epoch 118 with patience set to 50\n","early stopping at epoch 177 with patience set to 50\n","early stopping at epoch 130 with patience set to 50\n","early stopping at epoch 233 with patience set to 50\n","early stopping at epoch 87 with patience set to 50\n","early stopping at epoch 94 with patience set to 50\n","early stopping at epoch 75 with patience set to 50\n","early stopping at epoch 137 with patience set to 50\n","early stopping at epoch 126 with patience set to 50\n","100%|██████████| 50/50 [8:28:52<00:00, 610.66s/it, best loss: 0.18544238683127579]\n","the best GNN hyperparameters are: learning rate 0.0031622776601683794 | L2 regularization 0 | hidden_feats [128, 64] | dropout [0.1, 0.1] | predictor_hidden_feats 256 | predictor_dropout 0.4\n","reconstructing and training model on best hyperparameters\n","early stopping at epoch 125 with patience set to 50\n","performing repetitions on different seeds\n","early stopping at epoch 119 with patience set to 50\n","early stopping at epoch 155 with patience set to 50\n","early stopping at epoch 131 with patience set to 50\n","early stopping at epoch 157 with patience set to 50\n","early stopping at epoch 130 with patience set to 50\n","early stopping at epoch 110 with patience set to 50\n","early stopping at epoch 115 with patience set to 50\n","early stopping at epoch 194 with patience set to 50\n","early stopping at epoch 140 with patience set to 50\n","early stopping at epoch 234 with patience set to 50\n"]},{"ename":"ArrowTypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mArrowTypeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-cd76e9c5a563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0moverview_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverview_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0moverview_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverview_df_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model information added to experimentation overview\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverview_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m         )\n\u001b[1;32m   2687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mpartition_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m     )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preserve_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         arrays = [convert_column(c, f)\n\u001b[0;32m--> 595\u001b[0;31m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnthreads\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         arrays = [convert_column(c, f)\n\u001b[0;32m--> 595\u001b[0;31m                   for c, f in zip(columns_to_convert, convert_fields)]\n\u001b[0m\u001b[1;32m    596\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    579\u001b[0m             e.args += (\"Conversion failed for column {!s} with type {!s}\"\n\u001b[1;32m    580\u001b[0m                        .format(col.name, col.dtype),)\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfield_nullable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnull_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             raise ValueError(\"Field {} was non-nullable but pandas column \"\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mconvert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m         except (pa.ArrowInvalid,\n\u001b[1;32m    577\u001b[0m                 \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArrowNotImplementedError\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/array.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;31mArrowTypeError\u001b[0m: (\"Expected bytes, got a 'dict' object\", 'Conversion failed for column hyperparameters with type object')"]}],"source":["# gcn setup\n","model_name = \"gcn\" \n","hyper_space = hspace_gnns[model_name]\n","filename_gcn = \"gcn_opt\"\n","\n","if file_exists(filename_gcn+\".sav\") and file_exists(filename_gcn+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","    \n","    # model\n","    print('\\n')\n","    print(\"best GCN model is:\")\n","    loaded_model = pickle.load(open(filename_gcn+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_gcn + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_gcn = fmin(hyper_opt, hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    text = (\n","            \"the best GNN hyperparameters are: \"\n","            f\"learning rate {gcn_lr_ls[best_results_gcn['lr']]} | \"\n","            f\"L2 regularization {gcn_l2_ls[best_results_gcn['l2']]} | \"\n","            f\"hidden_feats {gcn_hidden_feats_ls[best_results_gcn['hidden_feats']]} | \"\n","            f\"dropout {gcn_dropout_ls[best_results_gcn['dropout']]} | \"\n","            f\"predictor_hidden_feats {gcn_predictor_hidden_feats_ls[best_results_gcn['predictor_hidden_feats']]} | \"\n","            f\"predictor_dropout {gcn_predictor_dropout_ls[best_results_gcn['predictor_dropout']]}\"\n","            )\n","    print(text)\n","    \n","    # reconstruct best model\n","    model_hp_opt = GCNPredictor(in_feats = node_feat_dim,\n","                              hidden_feats = gcn_hidden_feats_ls[best_results_gcn['hidden_feats']],\n","                              dropout = gcn_dropout_ls[best_results_gcn['dropout']],\n","                              predictor_hidden_feats = gcn_predictor_hidden_feats_ls[best_results_gcn['predictor_hidden_feats']],\n","                              predictor_dropout = gcn_predictor_dropout_ls[best_results_gcn['predictor_dropout']])\n","    \n","    print(\"reconstructing and training model on best hyperparameters\") \n","    # reconstruct best optimizer\n","    best_opt = optim.Adam(model_hp_opt.parameters(), lr=gcn_lr_ls[best_results_gcn[\"lr\"]], weight_decay=gcn_l2_ls[best_results_gcn[\"l2\"]])\n","\n","    # reset random seed\n","    seed=0\n","    torch.manual_seed(seed) \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed) \n","    \n","    # training\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","    # stopper\n","    model_hp_opt.to(device)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    best_auc = 0\n","    \n","    # training\n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","            \n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model_hp_opt)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","    \n","    # save hyperparameters\n","    with open(filename_gcn+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_gcn, f)\n","    # loadable via ...\n","    # with open(filename_gcn+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_gcn+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_gcn+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    # repetitions\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        torch.manual_seed(seed) \n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed) \n","        \n","        model_hp_opt = GCNPredictor(in_feats = node_feat_dim,\n","                                hidden_feats = gcn_hidden_feats_ls[best_results_gcn['hidden_feats']],\n","                                dropout = gcn_dropout_ls[best_results_gcn['dropout']],\n","                                predictor_hidden_feats = gcn_predictor_hidden_feats_ls[best_results_gcn['predictor_hidden_feats']],\n","                                predictor_dropout = gcn_predictor_dropout_ls[best_results_gcn['predictor_dropout']])\n","        \n","        # reconstruct best optimizer\n","        best_opt = optim.Adam(model_hp_opt.parameters(), lr=gcn_lr_ls[best_results_gcn[\"lr\"]], weight_decay=gcn_l2_ls[best_results_gcn[\"l2\"]])\n","\n","        # training\n","        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","        # stopper\n","        model_hp_opt.to(device)\n","\n","        #  early stopping\n","        increase_idx = 0\n","        best_auc = 0\n","        \n","        # training\n","        for j in range(epochs):\n","            \n","            # training\n","            train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","            # roc auc\n","            train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","            valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","            loss = 1 - valid_auc\n","                \n","            # checks if there was an update\n","            if valid_auc > best_auc:\n","                increase_idx = j\n","                # update valid_aicj\n","                best_auc = valid_auc\n","                # save model\n","                best_model = copy.deepcopy(model_hp_opt)\n","            # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","            # check how long there was no improvement\n","            if j - increase_idx >= patience:\n","                print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","                break        \n","\n","        # training metrics calc\n","        tr_metrics = list(eval_epoch_statistical(best_model, device, train_loader, evaluator))\n","\n","        # validation metric calc\n","        va_metrics = list(eval_epoch_statistical(best_model, device, valid_loader, evaluator))\n","\n","        # test metric calc\n","        te_metrics = list(eval_epoch_statistical(best_model, device, test_loader, evaluator))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    gcn_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    gcn_perf.to_parquet(filename_gcn + \"_performance\", index=0)      \n","    # loadable via ...\n","    # gcn_perf = pd.read_parquet(filename_gcn + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(gcn_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(gcn_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(gcn_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"GCN\"\n","    data_features = f\"molecule graphs with {node_feat_dim} node feats\" if model_name in [\"gcn\", \"gat\", \"nfp\"] else f\"molecule graphs with {node_feat_dim} node feats and {edge_feat_dim} edge feats\"\n","    filename = filename_gcn\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    \n","    hyperparameters = {\"learning rate\": gcn_lr_ls[best_results_gcn['lr']],\n","                       \"L2 regularization\": gcn_l2_ls[best_results_gcn['l2']],\n","                       \"hidden_feats\": gcn_hidden_feats_ls[best_results_gcn['hidden_feats']],\n","                       \"dropout\": gcn_dropout_ls[best_results_gcn['dropout']],\n","                       \"predictor_hidden_feats\": gcn_predictor_hidden_feats_ls[best_results_gcn['predictor_hidden_feats']],\n","                       \"predictor_dropout\": gcn_predictor_dropout_ls[best_results_gcn['predictor_dropout']],\n","                       \"other\": \"default\"\n","                       }\n","\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        print(\"saving information\")\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df[\"hyperparameters\"]= overview_df[\"hyperparameters\"].astype(str)\n","        overview_df.to_parquet(overview_df_filename, index=0)        \n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"xCn66vdMxyaH"},"source":["## AttentiveFP"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44002941,"status":"ok","timestamp":1659165630226,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"JRLlYmL1VPJL","outputId":"b59bb77d-456b-451f-b881-da6b4a65d34f"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","early stopping at epoch 81 with patience set to 50\n","early stopping at epoch 84 with patience set to 50\n","early stopping at epoch 54 with patience set to 50\n","early stopping at epoch 120 with patience set to 50\n","early stopping at epoch 127 with patience set to 50\n","early stopping at epoch 102 with patience set to 50\n","early stopping at epoch 53 with patience set to 50\n","early stopping at epoch 86 with patience set to 50\n","early stopping at epoch 94 with patience set to 50\n","early stopping at epoch 50 with patience set to 50\n","early stopping at epoch 51 with patience set to 50\n","early stopping at epoch 102 with patience set to 50\n","early stopping at epoch 112 with patience set to 50\n","early stopping at epoch 53 with patience set to 50\n","early stopping at epoch 73 with patience set to 50\n","early stopping at epoch 132 with patience set to 50\n","early stopping at epoch 72 with patience set to 50\n","early stopping at epoch 70 with patience set to 50\n","early stopping at epoch 53 with patience set to 50\n","early stopping at epoch 81 with patience set to 50\n","early stopping at epoch 74 with patience set to 50\n","early stopping at epoch 155 with patience set to 50\n","early stopping at epoch 78 with patience set to 50\n","early stopping at epoch 89 with patience set to 50\n","early stopping at epoch 89 with patience set to 50\n","early stopping at epoch 73 with patience set to 50\n","early stopping at epoch 74 with patience set to 50\n","early stopping at epoch 89 with patience set to 50\n","early stopping at epoch 92 with patience set to 50\n","100%|██████████| 30/30 [10:10:02<00:00, 1220.09s/it, best loss: 0.17943795316480504]\n","the best GNN hyperparameters are: learning rate 0.00031622776601683794 | L2 regularization 1e-08 | num_layers 5 | num_timesteps 3 | graph_feat_size 200 | dropout 0\n","reconstructing and training model on best hyperparameters\n","early stopping at epoch 64 with patience set to 50\n","performing repetitions on different seeds\n","early stopping at epoch 77 with patience set to 50\n","early stopping at epoch 76 with patience set to 50\n","early stopping at epoch 120 with patience set to 50\n","early stopping at epoch 80 with patience set to 50\n","early stopping at epoch 70 with patience set to 50\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                                      | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:---------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [9.9891e-01 8.0000e-05 9.9908e-01]      | [0.79425 0.00737 0.80282]               | [0.75037 0.00928 0.76298]              |\n","|  5 | XGB          | xgb_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7144328026415806, 'gamma': 0.1982639800431708, 'learning_rate': 0.1246187700498615, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 4, 'missing': None, 'n_estimators': 300, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 881799, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.7215172738554442, 'verbosity': 1}  | [0.89512 0.0249  0.92884]               | [0.7885  0.00918 0.80097]               | [0.76791 0.01247 0.77909]              |\n","|  6 | MLP          | mlp_opt_full_descr_50      | Tesla T4             | 27.33 GB | all features (233 feats)                           | {'activation': 'relu', 'alpha': 0.0002746800923919801, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.0031622776601683794, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 353006, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}       | [9.9970e-01 3.5000e-04 9.9998e-01]      | [0.7626  0.01725 0.79007]               | [0.70937 0.01347 0.73482]              |\n","|  7 | MLP          | mlp_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'activation': 'relu', 'alpha': 0.004791443737261613, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.03162277660168379, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 926051, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}               | [0.88876 0.01725 0.91386]               | [0.7595  0.03619 0.81185]               | [0.70339 0.0151  0.71833]              |\n","|  8 | GCN          | gcn_opt                    | Tesla P100-PCIE-16GB | 27.33 GB | molecule graphs with 9 node feats                  | {'learning rate': 0.0031622776601683794, 'L2 regularization': 0, 'hidden_feats': [128, 64], 'dropout': [0.1, 0.1], 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                     | [0.86342 0.01598 0.8936 ]               | [0.8131  0.00768 0.8264 ]               | [0.74874 0.01467 0.77431]              |\n","|  9 | AttentiveFP  | attfp_opt                  | Tesla T4             | 27.33 GB | molecule graphs with 9 node feats and 3 edge feats | {'learning rate': 0.00031622776601683794, 'L2 regularization': 1e-08, 'num_layers': 5, 'num_timesteps': 3, 'graph_feat_size': 200, 'dropout': 0, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                                                | [0.95324, 0.02554, 0.99696]             | [0.8302, 0.00828, 0.8451]               | [0.74323, 0.017, 0.7666]               |\n"]}],"source":["# attfp setup\n","model_name = \"attfp\" \n","hyper_space = hspace_gnns[model_name]\n","filename_attfp = \"attfp_opt\"\n","\n","if file_exists(filename_attfp+\".sav\") and file_exists(filename_attfp+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","    \n","    # model\n","    print('\\n')\n","    print(\"best AttentiveFP model is:\")\n","    loaded_model = pickle.load(open(filename_attfp+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_attfp + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_attfp = fmin(hyper_opt, hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    text = (\n","            \"the best GNN hyperparameters are: \"\n","            f\"learning rate {attfp_lr_ls[best_results_attfp['lr']]} | \"\n","            f\"L2 regularization {attfp_l2_ls[best_results_attfp['l2']]} | \"\n","            f\"num_layers {attfp_num_layers_ls[best_results_attfp['num_layers']]} | \"\n","            f\"num_timesteps {attfp_num_timesteps_ls[best_results_attfp['num_timesteps']]} | \"\n","            f\"graph_feat_size {attfp_graph_feat_size_ls[best_results_attfp['graph_feat_size']]} | \"\n","            f\"dropout {attfp_dropout_ls[best_results_attfp['dropout']]}\"\n","            )\n","    print(text)\n","    \n","    # reconstruct best model\n","    model_hp_opt = AttentiveFPPredictor(node_feat_size = node_feat_dim,\n","                                        edge_feat_size = edge_feat_dim, \n","                                        num_layers = attfp_num_layers_ls[best_results_attfp['num_layers']], \n","                                        num_timesteps = attfp_num_timesteps_ls[best_results_attfp['num_timesteps']],\n","                                        graph_feat_size = attfp_graph_feat_size_ls[best_results_attfp['graph_feat_size']], \n","                                        dropout = attfp_dropout_ls[best_results_attfp['dropout']])\n","    \n","    \n","    print(\"reconstructing and training model on best hyperparameters\") \n","    # reconstruct best optimizer\n","    best_opt = optim.Adam(model_hp_opt.parameters(), lr=attfp_lr_ls[best_results_attfp[\"lr\"]], weight_decay=attfp_l2_ls[best_results_attfp[\"l2\"]])\n","\n","    # reset random seed\n","    seed=0\n","    torch.manual_seed(seed) \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed) \n","    \n","    # training\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","    # stopper\n","    model_hp_opt.to(device)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    best_auc = 0\n","    \n","    # training\n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","            \n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model_hp_opt)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","    \n","    # save hyperparameters\n","    with open(filename_attfp+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_attfp, f)\n","    # loadable via ...\n","    # with open(filename_attfp+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_attfp+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_attfp+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    # repetitions\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        torch.manual_seed(seed) \n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed) \n","        \n","        # reconstruct best model\n","        model_hp_opt = AttentiveFPPredictor(node_feat_size = node_feat_dim,\n","                                            edge_feat_size = edge_feat_dim, \n","                                            num_layers = attfp_num_layers_ls[best_results_attfp['num_layers']], \n","                                            num_timesteps = attfp_num_timesteps_ls[best_results_attfp['num_timesteps']],\n","                                            graph_feat_size = attfp_graph_feat_size_ls[best_results_attfp['graph_feat_size']], \n","                                            dropout = attfp_dropout_ls[best_results_attfp['dropout']])\n","        \n","        # reconstruct best optimizer\n","        best_opt = optim.Adam(model_hp_opt.parameters(), lr=attfp_lr_ls[best_results_attfp[\"lr\"]], weight_decay=attfp_l2_ls[best_results_attfp[\"l2\"]])\n","\n","        # training\n","        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","        # stopper\n","        model_hp_opt.to(device)\n","\n","        #  early stopping\n","        increase_idx = 0\n","        best_auc = 0\n","        \n","        # training\n","        for j in range(epochs):\n","            \n","            # training\n","            train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","            # roc auc\n","            train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","            valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","            loss = 1 - valid_auc\n","                \n","            # checks if there was an update\n","            if valid_auc > best_auc:\n","                increase_idx = j\n","                # update valid_aicj\n","                best_auc = valid_auc\n","                # save model\n","                best_model = copy.deepcopy(model_hp_opt)\n","            # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","            # check how long there was no improvement\n","            if j - increase_idx >= patience:\n","                print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","                break        \n","\n","        # training metrics calc\n","        tr_metrics = list(eval_epoch_statistical(best_model, device, train_loader, evaluator))\n","\n","        # validation metric calc\n","        va_metrics = list(eval_epoch_statistical(best_model, device, valid_loader, evaluator))\n","\n","        # test metric calc\n","        te_metrics = list(eval_epoch_statistical(best_model, device, test_loader, evaluator))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    attfp_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    attfp_perf.to_parquet(filename_attfp + \"_performance\", index=0)      \n","    # loadable via ...\n","    # attfp_perf = pd.read_parquet(filename_attfp + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(attfp_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(attfp_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(attfp_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"AttentiveFP\"\n","    data_features = f\"molecule graphs with {node_feat_dim} node feats\" if model_name in [\"gcn\", \"gat\", \"nfp\"] else f\"molecule graphs with {node_feat_dim} node feats and {edge_feat_dim} edge feats\"\n","    filename = filename_attfp\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    \n","    hyperparameters = {\"learning rate\": attfp_lr_ls[best_results_attfp['lr']],\n","                       \"L2 regularization\": attfp_l2_ls[best_results_attfp['l2']],\n","                       \"num_layers\": attfp_num_layers_ls[best_results_attfp['num_layers']],\n","                       \"num_timesteps\": attfp_num_timesteps_ls[best_results_attfp['num_timesteps']],\n","                       \"graph_feat_size\": attfp_graph_feat_size_ls[best_results_attfp['graph_feat_size']],\n","                       \"dropout\": attfp_dropout_ls[best_results_attfp['dropout']],\n","                       \"other\": \"default\"\n","                       }\n","\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        print(\"saving information\")\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df[\"hyperparameters\"]= overview_df[\"hyperparameters\"].astype(str)        \n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"tM4ABJ2FxFC7"},"source":["## GAT"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27072777,"status":"ok","timestamp":1659220048181,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"E7D-3XSeVTyX","outputId":"f73d7d11-c23b-4cdf-b35c-dbe1a84d842a"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","early stopping at epoch 66 with patience set to 50\n","early stopping at epoch 115 with patience set to 50\n","early stopping at epoch 74 with patience set to 50\n","early stopping at epoch 70 with patience set to 50\n","early stopping at epoch 73 with patience set to 50\n","early stopping at epoch 95 with patience set to 50\n","early stopping at epoch 122 with patience set to 50\n","early stopping at epoch 97 with patience set to 50\n","early stopping at epoch 183 with patience set to 50\n","early stopping at epoch 103 with patience set to 50\n","early stopping at epoch 69 with patience set to 50\n","early stopping at epoch 139 with patience set to 50\n","early stopping at epoch 111 with patience set to 50\n","early stopping at epoch 143 with patience set to 50\n","early stopping at epoch 79 with patience set to 50\n","early stopping at epoch 80 with patience set to 50\n","early stopping at epoch 92 with patience set to 50\n","early stopping at epoch 141 with patience set to 50\n","early stopping at epoch 94 with patience set to 50\n","early stopping at epoch 84 with patience set to 50\n","early stopping at epoch 137 with patience set to 50\n","early stopping at epoch 114 with patience set to 50\n","early stopping at epoch 148 with patience set to 50\n","early stopping at epoch 119 with patience set to 50\n","early stopping at epoch 198 with patience set to 50\n","early stopping at epoch 125 with patience set to 50\n","early stopping at epoch 124 with patience set to 50\n","100%|██████████| 30/30 [6:17:38<00:00, 755.30s/it, best loss: 0.187248922202626]\n","the best GNN hyperparameters are: learning rate 0.00031622776601683794 | L2 regularization 0.0001 | hidden_feats [128, 128] | num_heads [4, 3] | predictor_hidden_feats 64 | predictor_dropout 0.2\n","reconstructing and training model on best hyperparameters\n","early stopping at epoch 183 with patience set to 50\n","performing repetitions on different seeds\n","early stopping at epoch 125 with patience set to 50\n","early stopping at epoch 77 with patience set to 50\n","early stopping at epoch 139 with patience set to 50\n","early stopping at epoch 81 with patience set to 50\n","early stopping at epoch 88 with patience set to 50\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                                      | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:---------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [9.9891e-01 8.0000e-05 9.9908e-01]      | [0.79425 0.00737 0.80282]               | [0.75037 0.00928 0.76298]              |\n","|  5 | XGB          | xgb_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7144328026415806, 'gamma': 0.1982639800431708, 'learning_rate': 0.1246187700498615, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 4, 'missing': None, 'n_estimators': 300, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 881799, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.7215172738554442, 'verbosity': 1}  | [0.89512 0.0249  0.92884]               | [0.7885  0.00918 0.80097]               | [0.76791 0.01247 0.77909]              |\n","|  6 | MLP          | mlp_opt_full_descr_50      | Tesla T4             | 27.33 GB | all features (233 feats)                           | {'activation': 'relu', 'alpha': 0.0002746800923919801, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.0031622776601683794, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 353006, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}       | [9.9970e-01 3.5000e-04 9.9998e-01]      | [0.7626  0.01725 0.79007]               | [0.70937 0.01347 0.73482]              |\n","|  7 | MLP          | mlp_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'activation': 'relu', 'alpha': 0.004791443737261613, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.03162277660168379, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 926051, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}               | [0.88876 0.01725 0.91386]               | [0.7595  0.03619 0.81185]               | [0.70339 0.0151  0.71833]              |\n","|  8 | GCN          | gcn_opt                    | Tesla P100-PCIE-16GB | 27.33 GB | molecule graphs with 9 node feats                  | {'learning rate': 0.0031622776601683794, 'L2 regularization': 0, 'hidden_feats': [128, 64], 'dropout': [0.1, 0.1], 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                     | [0.86342 0.01598 0.8936 ]               | [0.8131  0.00768 0.8264 ]               | [0.74874 0.01467 0.77431]              |\n","|  9 | AttentiveFP  | attfp_opt                  | Tesla T4             | 27.33 GB | molecule graphs with 9 node feats and 3 edge feats | {'learning rate': 0.00031622776601683794, 'L2 regularization': 1e-08, 'num_layers': 5, 'num_timesteps': 3, 'graph_feat_size': 200, 'dropout': 0, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                                                | [0.95324 0.02554 0.99696]               | [0.8302  0.00828 0.8451 ]               | [0.74323 0.017   0.7666 ]              |\n","| 10 | GAT          | gat_opt                    | Tesla P100-PCIE-16GB | 27.33 GB | molecule graphs with 9 node feats                  | {'learning rate': 0.00031622776601683794, 'L2 regularization': 0.0001, 'hidden_feats': [128, 128], 'num_heads': [4, 3], 'predictor_hidden_feats': 64, 'predictor_dropout': 0.2, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                 | [0.91613, 0.04314, 0.96545]             | [0.83593, 0.01061, 0.84914]             | [0.76741, 0.00913, 0.77527]            |\n"]}],"source":["# gat setup\n","model_name = \"gat\" \n","hyper_space = hspace_gnns[model_name]\n","filename_gat = \"gat_opt\"\n","\n","if file_exists(filename_gat+\".sav\") and file_exists(filename_gat+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","    \n","    # model\n","    print('\\n')\n","    print(\"best gat model is:\")\n","    loaded_model = pickle.load(open(filename_gat+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_gat + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_gat = fmin(hyper_opt, hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    text = (\n","            \"the best GNN hyperparameters are: \"\n","            f\"learning rate {gat_lr_ls[best_results_gat['lr']]} | \"\n","            f\"L2 regularization {gat_l2_ls[best_results_gat['l2']]} | \"\n","            f\"hidden_feats {gat_hidden_feats_ls[best_results_gat['hidden_feats']]} | \"\n","            f\"num_heads {gat_num_heads_ls[best_results_gat['num_heads']]} | \"\n","            f\"predictor_hidden_feats {gat_predictor_hidden_feats_ls[best_results_gat['predictor_hidden_feats']]} | \"\n","            f\"predictor_dropout {gat_predictor_dropout_ls[best_results_gat['predictor_dropout']]}\"\n","            )\n","    print(text)\n","    \n","    # reconstruct best model\n","    model_hp_opt =GATPredictor(in_feats  = node_feat_dim,\n","                               hidden_feats = gat_hidden_feats_ls[best_results_gat['hidden_feats']],\n","                               num_heads = gat_num_heads_ls[best_results_gat['num_heads']],\n","                               predictor_hidden_feats = gat_predictor_hidden_feats_ls[best_results_gat['predictor_hidden_feats']], \n","                               predictor_dropout= gat_predictor_dropout_ls[best_results_gat['predictor_dropout']])\n","    \n","    print(\"reconstructing and training model on best hyperparameters\") \n","    # reconstruct best optimizer\n","    best_opt = optim.Adam(model_hp_opt.parameters(), lr=gat_lr_ls[best_results_gat[\"lr\"]], weight_decay=gat_l2_ls[best_results_gat[\"l2\"]])\n","\n","    # reset random seed\n","    seed=0\n","    torch.manual_seed(seed) \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed) \n","    \n","    # training\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","    # stopper\n","    model_hp_opt.to(device)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    best_auc = 0\n","    \n","    # training\n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","            \n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model_hp_opt)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","    \n","    # save hyperparameters\n","    with open(filename_gat+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_gat, f)\n","    # loadable via .at\n","    # with open(filename_gat+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_gat+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_gat+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    # repetitions\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        torch.manual_seed(seed) \n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed) \n","        \n","        # reconstruct best model\n","        model_hp_opt =GATPredictor(in_feats  = node_feat_dim,\n","                                hidden_feats = gat_hidden_feats_ls[best_results_gat['hidden_feats']],\n","                                num_heads = gat_num_heads_ls[best_results_gat['num_heads']],\n","                                predictor_hidden_feats = gat_predictor_hidden_feats_ls[best_results_gat['predictor_hidden_feats']], \n","                                predictor_dropout= gat_predictor_dropout_ls[best_results_gat['predictor_dropout']]) \n","               \n","        # reconstruct best optimizer\n","        best_opt = optim.Adam(model_hp_opt.parameters(), lr=gat_lr_ls[best_results_gat[\"lr\"]], weight_decay=gat_l2_ls[best_results_gat[\"l2\"]])\n","\n","        # training\n","        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","        # stopper\n","        model_hp_opt.to(device)\n","\n","        #  early stopping\n","        increase_idx = 0\n","        best_auc = 0\n","        \n","        # training\n","        for j in range(epochs):\n","            \n","            # training\n","            train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","            # roc auc\n","            train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","            valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","            loss = 1 - valid_auc\n","                \n","            # checks if there was an update\n","            if valid_auc > best_auc:\n","                increase_idx = j\n","                # update valid_aicj\n","                best_auc = valid_auc\n","                # save model\n","                best_model = copy.deepcopy(model_hp_opt)\n","            # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","            # check how long there was no improvement\n","            if j - increase_idx >= patience:\n","                print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","                break        \n","\n","        # training metrics calc\n","        tr_metrics = list(eval_epoch_statistical(best_model, device, train_loader, evaluator))\n","\n","        # validation metric calc\n","        va_metrics = list(eval_epoch_statistical(best_model, device, valid_loader, evaluator))\n","\n","        # test metric calc\n","        te_metrics = list(eval_epoch_statistical(best_model, device, test_loader, evaluator))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    gat_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    gat_perf.to_parquet(filename_gat + \"_performance\", index=0)      \n","    # loadable via ...\n","    # gat_perf = pd.read_parquet(filename_gat + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(gat_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(gat_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(gat_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"GAT\"\n","    data_features = f\"molecule graphs with {node_feat_dim} node feats\" if model_name in [\"gcn\", \"gat\", \"nfp\"] else f\"molecule graphs with {node_feat_dim} node feats and {edge_feat_dim} edge feats\"\n","    filename = filename_gat\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    \n","    hyperparameters = {\"learning rate\": gat_lr_ls[best_results_gat['lr']],\n","                       \"L2 regularization\": gat_l2_ls[best_results_gat['l2']],\n","                       \"hidden_feats\": gat_hidden_feats_ls[best_results_gat['hidden_feats']],\n","                       \"num_heads\": gat_num_heads_ls[best_results_gat['num_heads']],\n","                       \"predictor_hidden_feats\": gat_predictor_hidden_feats_ls[best_results_gat['predictor_hidden_feats']],\n","                       \"predictor_dropout\": gat_predictor_dropout_ls[best_results_gat['predictor_dropout']],\n","                       \"other\": \"default\"\n","                       }\n","\n","\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        print(\"saving information\")\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df[\"hyperparameters\"]= overview_df[\"hyperparameters\"].astype(str)        \n","        overview_df.to_parquet(overview_df_filename, index=0)        \n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"TdlUn1ZbwuSA"},"source":["_______________________________________________________"]},{"cell_type":"markdown","metadata":{"id":"F7d0t3FLoK4n"},"source":["## MPNN"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15967394,"status":"ok","timestamp":1659329055776,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"9IAY9_HTVWlm","outputId":"ed834917-b137-4624-a26b-9011e7a4a88e"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","early stopping at epoch 116 with patience set to 50\n","  0%|          | 0/30 [26:59<?, ?it/s, best loss: ?]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 55 with patience set to 50\n","  3%|▎         | 1/30 [35:40<13:03:19, 1620.67s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 60 with patience set to 50\n","  7%|▋         | 2/30 [2:16:52<7:34:19, 973.55s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 50 with patience set to 50\n"," 10%|█         | 3/30 [3:06:19<24:46:26, 3303.20s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 50 with patience set to 50\n"," 13%|█▎        | 4/30 [3:19:56<22:53:32, 3169.71s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 55 with patience set to 50\n"," 17%|█▋        | 5/30 [3:38:10<16:07:00, 2320.83s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 72 with patience set to 50\n"," 20%|██        | 6/30 [3:55:40<12:41:28, 1903.68s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 173 with patience set to 50\n"," 23%|██▎       | 7/30 [4:33:22<10:22:46, 1624.64s/it, best loss: 0.33743018812463255]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 72 with patience set to 50\n"," 27%|██▋       | 8/30 [4:53:10<11:10:02, 1827.39s/it, best loss: 0.2304128698804624]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 50 with patience set to 50\n"," 30%|███       | 9/30 [5:04:42<9:29:41, 1627.69s/it, best loss: 0.2304128698804624]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 125 with patience set to 50\n"," 33%|███▎      | 10/30 [5:28:40<7:26:15, 1338.77s/it, best loss: 0.2304128698804624]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 114 with patience set to 50\n"," 37%|███▋      | 11/30 [6:29:12<7:13:32, 1369.09s/it, best loss: 0.22634969625710377]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 125 with patience set to 50\n"," 40%|████      | 12/30 [6:52:18<10:17:18, 2057.69s/it, best loss: 0.22634969625710377]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 50 with patience set to 50\n"," 43%|████▎     | 13/30 [7:22:48<8:45:16, 1853.89s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r 47%|████▋     | 14/30 [7:22:49<8:12:32, 1847.00s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 69 with patience set to 50\n"," 50%|█████     | 15/30 [8:12:58<8:11:27, 1965.85s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 156 with patience set to 50\n"," 53%|█████▎    | 16/30 [8:59:28<6:14:34, 1605.32s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 50 with patience set to 50\n"," 57%|█████▋    | 17/30 [9:10:39<7:04:58, 1961.44s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 54 with patience set to 50\n"," 60%|██████    | 18/30 [9:24:53<5:14:44, 1573.71s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 50 with patience set to 50\n"," 63%|██████▎   | 19/30 [9:45:22<4:08:53, 1357.63s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 100 with patience set to 50\n"," 67%|██████▋   | 20/30 [10:41:34<3:39:51, 1319.19s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 159 with patience set to 50\n"," 70%|███████   | 21/30 [11:11:04<4:50:17, 1935.23s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 128 with patience set to 50\n"," 73%|███████▎  | 22/30 [11:35:44<4:11:25, 1885.65s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 114 with patience set to 50\n"," 77%|███████▋  | 23/30 [12:39:45<3:25:47, 1763.91s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 152 with patience set to 50\n"," 80%|████████  | 24/30 [13:09:17<3:58:43, 2387.25s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 178 with patience set to 50\n"," 83%|████████▎ | 25/30 [13:30:11<3:03:31, 2202.37s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r 87%|████████▋ | 26/30 [13:30:11<2:07:50, 1917.69s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r 90%|█████████ | 27/30 [14:07:13<1:40:27, 2009.01s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 52 with patience set to 50\n"," 93%|█████████▎| 28/30 [14:57:52<1:09:58, 2099.23s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 96 with patience set to 50\n"," 97%|█████████▋| 29/30 [15:15:46<28:08, 1688.39s/it, best loss: 0.20944481187536756]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","\n"]},{"name":"stdout","output_type":"stream","text":["100%|██████████| 30/30 [15:15:47<00:00, 1831.57s/it, best loss: 0.20944481187536756]\n","the best GNN hyperparameters are: learning rate 0.0031622776601683794 | L2 regularization 0.0001 | node_out_feats 16 | edge_hidden_feats 16 | num_step_message_passing 2 | num_step_set2set 4\n","reconstructing and training model on best hyperparameters\n","performing repetitions on different seeds\n","early stopping at epoch 94 with patience set to 50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 95 with patience set to 50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 161 with patience set to 50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n"]},{"name":"stdout","output_type":"stream","text":["early stopping at epoch 194 with patience set to 50\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:951: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:770: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n","  self.dropout, self.training, self.bidirectional, self.batch_first)\n"]},{"name":"stdout","output_type":"stream","text":["model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                                      | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:---------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)                           | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [9.9891e-01 8.0000e-05 9.9908e-01]      | [0.79425 0.00737 0.80282]               | [0.75037 0.00928 0.76298]              |\n","|  5 | XGB          | xgb_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7144328026415806, 'gamma': 0.1982639800431708, 'learning_rate': 0.1246187700498615, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 4, 'missing': None, 'n_estimators': 300, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 881799, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.7215172738554442, 'verbosity': 1}  | [0.89512 0.0249  0.92884]               | [0.7885  0.00918 0.80097]               | [0.76791 0.01247 0.77909]              |\n","|  6 | MLP          | mlp_opt_full_descr_50      | Tesla T4             | 27.33 GB | all features (233 feats)                           | {'activation': 'relu', 'alpha': 0.0002746800923919801, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.0031622776601683794, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 353006, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}       | [9.9970e-01 3.5000e-04 9.9998e-01]      | [0.7626  0.01725 0.79007]               | [0.70937 0.01347 0.73482]              |\n","|  7 | MLP          | mlp_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats)                       | {'activation': 'relu', 'alpha': 0.004791443737261613, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.03162277660168379, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 926051, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}               | [0.88876 0.01725 0.91386]               | [0.7595  0.03619 0.81185]               | [0.70339 0.0151  0.71833]              |\n","|  8 | GCN          | gcn_opt                    | Tesla P100-PCIE-16GB | 27.33 GB | molecule graphs with 9 node feats                  | {'learning rate': 0.0031622776601683794, 'L2 regularization': 0, 'hidden_feats': [128, 64], 'dropout': [0.1, 0.1], 'predictor_hidden_feats': 256, 'predictor_dropout': 0.4, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                     | [0.86342 0.01598 0.8936 ]               | [0.8131  0.00768 0.8264 ]               | [0.74874 0.01467 0.77431]              |\n","|  9 | AttentiveFP  | attfp_opt                  | Tesla T4             | 27.33 GB | molecule graphs with 9 node feats and 3 edge feats | {'learning rate': 0.00031622776601683794, 'L2 regularization': 1e-08, 'num_layers': 5, 'num_timesteps': 3, 'graph_feat_size': 200, 'dropout': 0, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                                                | [0.95324 0.02554 0.99696]               | [0.8302  0.00828 0.8451 ]               | [0.74323 0.017   0.7666 ]              |\n","| 10 | GAT          | gat_opt                    | Tesla P100-PCIE-16GB | 27.33 GB | molecule graphs with 9 node feats                  | {'learning rate': 0.00031622776601683794, 'L2 regularization': 0.0001, 'hidden_feats': [128, 128], 'num_heads': [4, 3], 'predictor_hidden_feats': 64, 'predictor_dropout': 0.2, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                 | [0.91613 0.04314 0.96545]               | [0.83593 0.01061 0.84914]               | [0.76741 0.00913 0.77527]              |\n","| 11 | MPNN         | mpnn_opt                   | Tesla P100-PCIE-16GB | 27.33 GB | molecule graphs with 9 node feats and 3 edge feats | {'learning rate': 0.0031622776601683794, 'L2 regularization': 0.0001, 'node_out_feats': 16, 'edge_hidden_feats': 16, 'num_step_message_passing': 2, 'num_step_set2set': 4, 'other': 'default'}                                                                                                                                                                                                                                                                                                                                                      | [0.83593, 0.01421, 0.85346]             | [0.78624, 0.01198, 0.80404]             | [0.73589, 0.02243, 0.76032]            |\n"]}],"source":["# mpnn setup\n","model_name = \"mpnn\"\n","hyper_space = hspace_gnns[model_name]\n","filename_mpnn = \"mpnn_opt\"\n","\n","if file_exists(filename_mpnn+\".sav\") and file_exists(filename_mpnn+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","    \n","    # model\n","    print('\\n')\n","    print(\"best MPNN model is:\")\n","    loaded_model = pickle.load(open(filename_mpnn +\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_mpnn + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_mpnn = fmin(hyper_opt, hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    text = (\n","            \"the best GNN hyperparameters are: \"\n","            f\"learning rate {mpnn_lr_ls[best_results_mpnn['lr']]} | \"\n","            f\"L2 regularization {mpnn_l2_ls[best_results_mpnn['l2']]} | \"\n","            f\"node_out_feats {mpnn_node_out_feats_ls[best_results_mpnn['node_out_feats']]} | \"\n","            f\"edge_hidden_feats {mpnn_edge_hidden_feats_ls[best_results_mpnn['edge_hidden_feats']]} | \"\n","            f\"num_step_message_passing {mpnn_num_step_message_passing_ls[best_results_mpnn['num_step_message_passing']]} | \"\n","            f\"num_step_set2set {mpnn_num_step_set2set_ls[best_results_mpnn['num_step_set2set']]}\"\n","            )\n","\n","    print(text)\n","    \n","    # reconstruct best model\n","    model_hp_opt = MPNNPredictor(node_in_feats = node_feat_dim ,\n","                                 edge_in_feats = edge_feat_dim,\n","                                 node_out_feats = mpnn_node_out_feats_ls[best_results_mpnn['node_out_feats']],\n","                                 edge_hidden_feats = mpnn_edge_hidden_feats_ls[best_results_mpnn['edge_hidden_feats']],\n","                                 num_step_message_passing  = mpnn_num_step_message_passing_ls[best_results_mpnn['num_step_message_passing']],\n","                                 num_step_set2set = mpnn_num_step_set2set_ls[best_results_mpnn['num_step_set2set']]\n","                                 )\n","    \n","    print(\"reconstructing and training model on best hyperparameters\") \n","    # reconstruct best optimizer\n","    best_opt = optim.Adam(model_hp_opt.parameters(), lr=mpnn_lr_ls[best_results_mpnn[\"lr\"]], weight_decay=mpnn_l2_ls[best_results_mpnn[\"l2\"]])\n","\n","    # reset random seed\n","    seed=0\n","    torch.manual_seed(seed) \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed) \n","    \n","    # training\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","    # stopper\n","    model_hp_opt.to(device)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    best_auc = 0\n","    \n","    # training\n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","            \n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model_hp_opt)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","    \n","    # save hyperparameters\n","    with open(filename_mpnn+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_mpnn, f)\n","    # loadable via ...\n","    # with open(filename_mpnn+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_mpnn+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_mpnn+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    # repetitions\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        torch.manual_seed(seed) \n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed) \n","        \n","        # reconstruct best model\n","        model_hp_opt = MPNNPredictor(node_in_feats = node_feat_dim ,\n","                                    edge_in_feats = edge_feat_dim,\n","                                    node_out_feats = mpnn_node_out_feats_ls[best_results_mpnn['node_out_feats']],\n","                                    edge_hidden_feats = mpnn_edge_hidden_feats_ls[best_results_mpnn['edge_hidden_feats']],\n","                                    num_step_message_passing  = mpnn_num_step_message_passing_ls[best_results_mpnn['num_step_message_passing']],\n","                                    num_step_set2set = mpnn_num_step_set2set_ls[best_results_mpnn['num_step_set2set']]\n","                                    )      \n","          \n","        # reconstruct best optimizer\n","        best_opt = optim.Adam(model_hp_opt.parameters(), lr=mpnn_lr_ls[best_results_mpnn[\"lr\"]], weight_decay=mpnn_l2_ls[best_results_mpnn[\"l2\"]])\n","\n","        # training\n","        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","        # stopper\n","        model_hp_opt.to(device)\n","\n","        #  early stopping\n","        increase_idx = 0\n","        best_auc = 0\n","        \n","        # training\n","        for j in range(epochs):\n","            \n","            # training\n","            train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","            # roc auc\n","            train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","            valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","            loss = 1 - valid_auc\n","                \n","            # checks if there was an update\n","            if valid_auc > best_auc:\n","                increase_idx = j\n","                # update valid_aicj\n","                best_auc = valid_auc\n","                # save model\n","                best_model = copy.deepcopy(model_hp_opt)\n","            # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","            # check how long there was no improvement\n","            if j - increase_idx >= patience:\n","                print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","                break        \n","\n","        # training metrics calc\n","        tr_metrics = list(eval_epoch_statistical(best_model, device, train_loader, evaluator))\n","\n","        # validation metric calc\n","        va_metrics = list(eval_epoch_statistical(best_model, device, valid_loader, evaluator))\n","\n","        # test metric calc\n","        te_metrics = list(eval_epoch_statistical(best_model, device, test_loader, evaluator))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    mpnn_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    mpnn_perf.to_parquet(filename_mpnn + \"_performance\", index=0)      \n","    # loadable via ...\n","    # mpnn_perf = pd.read_parquet(filename_mpnn + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(mpnn_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(mpnn_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(mpnn_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"MPNN\"\n","    data_features = f\"molecule graphs with {node_feat_dim} node feats\" if model_name in [\"gcn\", \"gat\", \"nfp\"] else f\"molecule graphs with {node_feat_dim} node feats and {edge_feat_dim} edge feats\"\n","    filename = filename_mpnn\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    \n","    hyperparameters = {\"learning rate\": mpnn_lr_ls[best_results_mpnn['lr']],\n","                       \"L2 regularization\": mpnn_l2_ls[best_results_mpnn['l2']],\n","                       \"node_out_feats\": mpnn_node_out_feats_ls[best_results_mpnn['node_out_feats']],\n","                       \"edge_hidden_feats\": mpnn_edge_hidden_feats_ls[best_results_mpnn['edge_hidden_feats']],\n","                       \"num_step_message_passing\": mpnn_num_step_message_passing_ls[best_results_mpnn['num_step_message_passing']],\n","                       \"num_step_set2set\": mpnn_num_step_set2set_ls[best_results_mpnn['num_step_set2set']],\n","                       \"other\": \"default\"\n","                       }\n","\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        print(\"saving information\")\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df[\"hyperparameters\"]= overview_df[\"hyperparameters\"].astype(str)        \n","        overview_df.to_parquet(overview_df_filename, index=0)        \n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"8SKRF1poqcgL"},"source":["## Weave"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"X_0NhEsKVbG5","outputId":"12843fb3-6697-4e2d-e4de-526d3c3d3040"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","early stopping at epoch 51 with patience set to 50\n","early stopping at epoch 50 with patience set to 50\n","early stopping at epoch 88 with patience set to 50\n","early stopping at epoch 58 with patience set to 50\n","early stopping at epoch 140 with patience set to 50\n","early stopping at epoch 132 with patience set to 50\n","early stopping at epoch 132 with patience set to 50\n","early stopping at epoch 102 with patience set to 50\n","early stopping at epoch 185 with patience set to 50\n","early stopping at epoch 53 with patience set to 50\n","early stopping at epoch 70 with patience set to 50\n","early stopping at epoch 51 with patience set to 50\n","early stopping at epoch 169 with patience set to 50\n","early stopping at epoch 56 with patience set to 50\n","early stopping at epoch 126 with patience set to 50\n","early stopping at epoch 176 with patience set to 50\n","early stopping at epoch 147 with patience set to 50\n","early stopping at epoch 147 with patience set to 50\n"," 77%|███████▋  | 23/30 [7:14:05<2:30:41, 1291.62s/it, best loss: 0.1661308299039782]"]}],"source":["# weave setup\n","model_name = \"weave\" \n","hyper_space = hspace_gnns[model_name]\n","filename_weave = \"weave_opt\"\n","\n","if file_exists(filename_weave+\".sav\") and file_exists(filename_weave+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","    \n","    # model\n","    print('\\n')\n","    print(\"best Weave model is:\")\n","    loaded_model = pickle.load(open(filename_weave+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_weave + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_weave = fmin(hyper_opt, hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    text = (\n","            \"the best GNN hyperparameters are: \"\n","            f\"learning rate {weave_lr_ls[best_results_weave['lr']]} | \"\n","            f\"L2 regularization {weave_l2_ls[best_results_weave['l2']]} | \"\n","            f\"gnn_hidden_feats {weave_gnn_hidden_feats_ls[best_results_weave['gnn_hidden_feats']]} | \"\n","            f\"num_gnn_layers {weave_num_gnn_layers_ls[best_results_weave['num_gnn_layers']]} | \"\n","            f\"graph_feats {weave_graph_feats_ls[best_results_weave['graph_feats']]}\"\n","            )\n","    \n","    print(text)\n","    \n","    # reconstruct best model\n","    model_hp_opt = WeavePredictor(node_in_feats  = node_feat_dim,\n","                                  edge_in_feats =  edge_feat_dim,\n","                                  num_gnn_layers = weave_num_gnn_layers_ls[best_results_weave['num_gnn_layers']],\n","                                  gnn_hidden_feats = weave_gnn_hidden_feats_ls[best_results_weave['gnn_hidden_feats']],\n","                                  graph_feats = weave_graph_feats_ls[best_results_weave['graph_feats']])  \n","      \n","    print(\"reconstructing and training model on best hyperparameters\") \n","    # reconstruct best optimizer\n","    best_opt = optim.Adam(model_hp_opt.parameters(), lr=weave_lr_ls[best_results_weave[\"lr\"]], weight_decay=weave_l2_ls[best_results_weave[\"l2\"]])\n","\n","    # reset random seed\n","    seed=0\n","    torch.manual_seed(seed) \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed) \n","    \n","    # training\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","    # stopper\n","    model_hp_opt.to(device)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    best_auc = 0\n","    \n","    # training\n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","            \n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model_hp_opt)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","    \n","    # save hyperparameters\n","    with open(filename_weave+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_weave, f)\n","    # loadable via ...\n","    # with open(filename_weave+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_weave+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_weave+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    # repetitions\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        torch.manual_seed(seed) \n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed) \n","        \n","        model_hp_opt = WeavePredictor(node_in_feats  = node_feat_dim,\n","                        edge_in_feats =  edge_feat_dim,\n","                        num_gnn_layers = weave_num_gnn_layers_ls[best_results_weave['num_gnn_layers']],\n","                        gnn_hidden_feats = weave_gnn_hidden_feats_ls[best_results_weave['gnn_hidden_feats']],\n","                        graph_feats = weave_graph_feats_ls[best_results_weave['graph_feats']])  \n","        \n","        # reconstruct best optimizer\n","        best_opt = optim.Adam(model_hp_opt.parameters(), lr=weave_lr_ls[best_results_weave[\"lr\"]], weight_decay=weave_l2_ls[best_results_weave[\"l2\"]])\n","\n","        # training\n","        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","        # stopper\n","        model_hp_opt.to(device)\n","\n","        #  early stopping\n","        increase_idx = 0\n","        best_auc = 0\n","        \n","        # training\n","        for j in range(epochs):\n","            \n","            # training\n","            train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","            # roc auc\n","            train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","            valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","            loss = 1 - valid_auc\n","                \n","            # checks if there was an update\n","            if valid_auc > best_auc:\n","                increase_idx = j\n","                # update valid_aicj\n","                best_auc = valid_auc\n","                # save model\n","                best_model = copy.deepcopy(model_hp_opt)\n","            # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","            # check how long there was no improvement\n","            if j - increase_idx >= patience:\n","                print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","                break        \n","\n","        # training metrics calc\n","        tr_metrics = list(eval_epoch_statistical(best_model, device, train_loader, evaluator))\n","\n","        # validation metric calc\n","        va_metrics = list(eval_epoch_statistical(best_model, device, valid_loader, evaluator))\n","\n","        # test metric calc\n","        te_metrics = list(eval_epoch_statistical(best_model, device, test_loader, evaluator))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    weave_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    weave_perf.to_parquet(filename_weave + \"_performance\", index=0)      \n","    # loadable via ...\n","    # weave_perf = pd.read_parquet(filename_weave + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(weave_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(weave_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(weave_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"Weave\"\n","    data_features = f\"molecule graphs with {node_feat_dim} node feats\" if model_name in [\"gcn\", \"gat\", \"nfp\"] else f\"molecule graphs with {node_feat_dim} node feats and {edge_feat_dim} edge feats\"\n","    filename = filename_weave\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    \n","    hyperparameters = {\"learning rate\": weave_lr_ls[best_results_weave['lr']],\n","                       \"L2 regularization\": weave_l2_ls[best_results_weave['l2']],\n","                       \"gnn_hidden_feats\": weave_gnn_hidden_feats_ls[best_results_weave['gnn_hidden_feats']],\n","                       \"num_gnn_layers\": weave_num_gnn_layers_ls[best_results_weave['num_gnn_layers']],\n","                       \"graph_feats\": weave_graph_feats_ls[best_results_weave['graph_feats']],\n","                       \"other\": \"default\"\n","                       }\n","\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        print(\"saving information\")\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df[\"hyperparameters\"]= overview_df[\"hyperparameters\"].astype(str)\n","        overview_df.to_parquet(overview_df_filename, index=0)        \n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"e8YX3fXivP0W"},"source":["## NFP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcCA0lWOVkYj"},"outputs":[],"source":["# nfp setup\n","model_name = \"nfp\" \n","hyper_space = hspace_gnns[model_name]\n","filename_nfp = \"nfp_opt\"\n","\n","if file_exists(filename_nfp+\".sav\") and file_exists(filename_nfp+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","    \n","    # model\n","    print('\\n')\n","    print(\"best NFP model is:\")\n","    loaded_model = pickle.load(open(filename_nfp+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_nfp + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_nfp = fmin(hyper_opt, hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    text = (\n","            \"the best GNN hyperparameters are: \"\n","            f\"learning rate {nfp_lr_ls[best_results_nfp['lr']]} | \"\n","            f\"L2 regularization {nfp_l2_ls[best_results_nfp['l2']]} | \"\n","            f\"hidden_feats {nfp_hidden_feats_ls[best_results_nfp['hidden_feats']]} | \"\n","            f\"dropout {nfp_dropout_ls[best_results_nfp['dropout']]} | \"\n","            f\"predictor_hidden_size {nfp_predictor_hidden_size_ls[best_results_nfp['predictor_hidden_size']]} | \"\n","            f\"predictor_dropout {nfp_predictor_dropout_ls[best_results_nfp['predictor_dropout']]}\"\n","            )\n","    print(text)\n","    \n","    # reconstruct best model\n","    model_hp_opt = NFPredictor(in_feats = node_feat_dim,\n","                               hidden_feats = nfp_hidden_feats_ls[best_results_nfp['hidden_feats']],\n","                               dropout = nfp_dropout_ls[best_results_nfp['dropout']],\n","                               predictor_hidden_size = nfp_predictor_hidden_size_ls[best_results_nfp['predictor_hidden_size']],\n","                               predictor_dropout = nfp_predictor_dropout_ls[best_results_nfp['predictor_dropout']]\n","                               )\n","\n","\n","    print(\"reconstructing and training model on best hyperparameters\") \n","    # reconstruct best optimizer\n","    best_opt = optim.Adam(model_hp_opt.parameters(), lr=nfp_lr_ls[best_results_nfp[\"lr\"]], weight_decay=nfp_l2_ls[best_results_nfp[\"l2\"]])\n","\n","    # reset random seed\n","    seed=0\n","    torch.manual_seed(seed) \n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed) \n","    \n","    # training\n","    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","    # stopper\n","    model_hp_opt.to(device)\n","\n","    #  early stopping\n","    increase_idx = 0\n","    best_auc = 0\n","    \n","    # training\n","    for j in range(epochs):\n","        \n","        # training\n","        train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","        # roc auc\n","        train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","        valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","        loss = 1 - valid_auc\n","            \n","        # checks if there was an update\n","        if valid_auc > best_auc:\n","            increase_idx = j\n","            # update valid_aicj\n","            best_auc = valid_auc\n","            # save model\n","            best_model = copy.deepcopy(model_hp_opt)\n","        # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","        # check how long there was no improvement\n","        if j - increase_idx >= patience:\n","            print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","            break\n","    \n","    # save hyperparameters\n","    with open(filename_nfp+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_nfp, f)\n","    # loadable via ...\n","    # with open(filename_nfp+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_nfp+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_nfp+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    # repetitions\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        torch.manual_seed(seed) \n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed(seed) \n","                \n","        # reconstruct best model\n","        model_hp_opt = NFPredictor(in_feats = node_feat_dim,\n","                                hidden_feats = nfp_hidden_feats_ls[best_results_nfp['hidden_feats']],\n","                                dropout = nfp_dropout_ls[best_results_nfp['dropout']],\n","                                predictor_hidden_size = nfp_predictor_hidden_size_ls[best_results_nfp['predictor_hidden_size']],\n","                                predictor_dropout = nfp_predictor_dropout_ls[best_results_nfp['predictor_dropout']]\n","                                )\n","\n","\n","        # reconstruct best optimizer\n","        best_opt = optim.Adam(model_hp_opt.parameters(), lr=nfp_lr_ls[best_results_nfp[\"lr\"]], weight_decay=nfp_l2_ls[best_results_nfp[\"l2\"]])\n","\n","        # training\n","        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n","        # stopper\n","        model_hp_opt.to(device)\n","\n","        #  early stopping\n","        increase_idx = 0\n","        best_auc = 0\n","        \n","        # training\n","        for j in range(epochs):\n","            \n","            # training\n","            train_loss = train_epoch(model_hp_opt, device, train_loader, best_opt, loss_fn)\n","\n","            # roc auc\n","            train_auc = eval_epoch(model_hp_opt, device, train_loader, evaluator)\n","            valid_auc = eval_epoch(model_hp_opt, device, valid_loader, evaluator)\n","\n","            loss = 1 - valid_auc\n","                \n","            # checks if there was an update\n","            if valid_auc > best_auc:\n","                increase_idx = j\n","                # update valid_aicj\n","                best_auc = valid_auc\n","                # save model\n","                best_model = copy.deepcopy(model_hp_opt)\n","            # print(f\"epoch {j}, valid_auc {valid_auc}, best_auc {best_auc}, epochs with no impr {j - increase_idx}\")\n","            # check how long there was no improvement\n","            if j - increase_idx >= patience:\n","                print(f\"early stopping at epoch {j} with patience set to {patience}\")\n","                break        \n","\n","        # training metrics calc\n","        tr_metrics = list(eval_epoch_statistical(best_model, device, train_loader, evaluator))\n","\n","        # validation metric calc\n","        va_metrics = list(eval_epoch_statistical(best_model, device, valid_loader, evaluator))\n","\n","        # test metric calc\n","        te_metrics = list(eval_epoch_statistical(best_model, device, test_loader, evaluator))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    nfp_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    nfp_perf.to_parquet(filename_nfp + \"_performance\", index=0)      \n","    # loadable via ...\n","    # nfp_perf = pd.read_parquet(filename_nfp + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(nfp_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(nfp_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(nfp_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"NFP\"\n","    data_features = f\"molecule graphs with {node_feat_dim} node feats\" if model_name in [\"gcn\", \"gat\", \"nfp\"] else f\"molecule graphs with {node_feat_dim} node feats and {edge_feat_dim} edge feats\"\n","    filename = filename_nfp\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    \n","    hyperparameters = {\"learning rate\": nfp_lr_ls[best_results_nfp['lr']],\n","                       \"L2 regularization\": nfp_l2_ls[best_results_nfp['l2']],\n","                       \"hidden_feats\": nfp_hidden_feats_ls[best_results_nfp['hidden_feats']],\n","                       \"dropout\": nfp_dropout_ls[best_results_nfp['dropout']],\n","                       \"predictor_hidden_size\": nfp_predictor_hidden_size_ls[best_results_nfp['predictor_hidden_size']],\n","                       \"predictor_dropout\": nfp_predictor_dropout_ls[best_results_nfp['predictor_dropout']],\n","                       \"other\": \"default\"\n","                       }\n","\n","\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        print(\"saving information\")\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df[\"hyperparameters\"]= overview_df[\"hyperparameters\"].astype(str)\n","        overview_df.to_parquet(overview_df_filename, index=0)        \n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["G-Pou2-ss7b0","ZPm0WWxGtDCP"],"machine_shape":"hm","name":"1_train_GNNs.ipynb","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.13 ('g_vs_t')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.13"},"vscode":{"interpreter":{"hash":"60493d801fb9e55f2d7bb7f955358396b43e462d1fc05c2a7271c1b502058613"}}},"nbformat":4,"nbformat_minor":0}
