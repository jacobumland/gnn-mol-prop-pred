{"cells":[{"cell_type":"markdown","metadata":{},"source":["*this notebook was run in Google Colab*"]},{"cell_type":"markdown","metadata":{"id":"6ZFmQ0tI24C6"},"source":["# setup"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1716,"status":"ok","timestamp":1659002683468,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"u3-oUaoQ2y21","outputId":"de045748-0a72-41a9-d580-1fb236d35e36"},"outputs":[{"name":"stdout","output_type":"stream","text":["Colab: mounting Google drive on  /content/gdrive\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","\n","Colab: making sure  /content/gdrive/My Drive/Colab Notebooks/thesis_training_models  exists.\n","\n","Colab: Changing directory to  /content/gdrive/My Drive/Colab Notebooks/thesis_training_models\n","/content/gdrive/My Drive/Colab Notebooks/thesis_training_models\n"]}],"source":["# mount notebook\n","from google.colab import drive\n","\n","mount='/content/gdrive'\n","print(\"Colab: mounting Google drive on \", mount)\n","\n","drive.mount(mount)\n","\n","# switch to the directory on the Google Drive that you want to use\n","import os\n","drive_root = mount + \"/My Drive/Colab Notebooks/thesis_training_models\"\n","  \n","# create drive_root if it doesn't exist\n","create_drive_root = True\n","if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","\n","# change to the directory\n","print(\"\\nColab: Changing directory to \", drive_root)\n","%cd $drive_root"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1659002683469,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"LmfXGSeP2-1z","outputId":"720ba4d3-ff0f-4239-9b74-bb3a20049c61"},"outputs":[{"name":"stdout","output_type":"stream","text":["Thu Jul 28 10:04:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# check computational resources: GPU\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1659002683469,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"riX3GgPp3F-M","outputId":"7355189e-b47e-454b-f2dc-0c70a5c4800f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Your runtime has 27.3 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}],"source":["# check computational resources: RAM\n","from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7319,"status":"ok","timestamp":1659002690784,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"r6Kgp3-p-QnF","outputId":"298f11d6-f7d5-4cfe-dbc8-136618982c6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.3)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.0+cu113)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n","Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n","Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://data.dgl.ai/wheels/repo.html\n","Requirement already satisfied: dgl-cu113 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n","Requirement already satisfied: dglgo in /usr/local/lib/python3.7/dist-packages (0.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (4.64.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.21.6)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (5.9.1)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (1.7.3)\n","Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.6.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl-cu113) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl-cu113) (2.10)\n","Requirement already satisfied: typer>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (0.4.2)\n","Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.7/dist-packages (from dglgo) (2022.3.4)\n","Requirement already satisfied: ruamel.yaml>=0.17.20 in /usr/local/lib/python3.7/dist-packages (from dglgo) (0.17.21)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.7/dist-packages (from dglgo) (6.0)\n","Requirement already satisfied: ogb>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.3.3)\n","Requirement already satisfied: pydantic>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.9.1)\n","Requirement already satisfied: autopep8>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.6.0)\n","Requirement already satisfied: numpydoc>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.4.0)\n","Requirement already satisfied: isort>=5.10.1 in /usr/local/lib/python3.7/dist-packages (from dglgo) (5.10.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from dglgo) (1.0.2)\n","Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from autopep8>=1.6.0->dglgo) (0.10.2)\n","Requirement already satisfied: pycodestyle>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from autopep8>=1.6.0->dglgo) (2.8.0)\n","Requirement already satisfied: sphinx>=3.0 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (5.1.1)\n","Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.7/dist-packages (from numpydoc>=1.1.0->dglgo) (2.11.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.10->numpydoc>=1.1.0->dglgo) (2.0.1)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb>=1.3.3->dglgo) (1.3.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb>=1.3.3->dglgo) (1.15.0)\n","Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb>=1.3.3->dglgo) (0.2.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb>=1.3.3->dglgo) (1.12.0+cu113)\n","Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb>=1.3.3->dglgo) (0.2.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb>=1.3.3->dglgo) (2.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.9.0->dglgo) (4.1.1)\n","Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml>=0.17.20->dglgo) (0.2.6)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->dglgo) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->dglgo) (3.1.0)\n","Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.0.2)\n","Requirement already satisfied: docutils<0.20,>=0.14 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (0.17.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (21.3)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.4.1)\n","Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.0.3)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.2.0)\n","Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.0.1)\n","Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.0.2)\n","Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (1.1.5)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.6.1)\n","Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.0.0)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (0.7.12)\n","Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (2.10.3)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from sphinx>=3.0->numpydoc>=1.1.0->dglgo) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->sphinx>=3.0->numpydoc>=1.1.0->dglgo) (3.8.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer>=0.4.0->dglgo) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=3.0->numpydoc>=1.1.0->dglgo) (3.0.9)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from rdkit-pypi->dglgo) (7.1.2)\n"]}],"source":["# installations\n","! pip install ogb\n","\n","# dgl\n","!pip install dgl-cu113 dglgo -f https://data.dgl.ai/wheels/repo.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8vQAT7C-eBS"},"outputs":[],"source":["# imports\n","import dgl\n","\n","from dgl import backend as F\n","\n","import pandas as pd\n","\n","from ogb.graphproppred import DglGraphPropPredDataset\n","\n","from sklearn.svm import SVC\n","\n","from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n","\n","from sklearn.metrics import roc_auc_score, confusion_matrix, precision_recall_curve, auc\n","\n","import numpy as np\n","\n","import pickle\n","\n","import re\n","\n","from tensorflow.python.client import device_lib\n","\n","from os.path import exists as file_exists\n","\n","from sklearn.feature_selection import SelectPercentile, f_classif, SelectFromModel\n","\n","from xgboost import XGBClassifier\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from sklearn.neural_network import MLPClassifier"]},{"cell_type":"markdown","metadata":{"id":"gMLtmHr9-rvj"},"source":["# helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKy2yoPq-r7N"},"outputs":[],"source":["def get_split_data(input_path):\n","\n","    # load data\n","    dataset = DglGraphPropPredDataset(name = \"ogbg-molhiv\")\n","\n","    # load processed data\n","    df = pd.read_csv(\"./dataset/1D_2D_PubChemFP_SubFP_preprocessed.csv\")\n","\n","    # load splitting indices with OGB scaffold splitting\n","    split_idx = dataset.get_idx_split()\n","    train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n","\n","    return  df.iloc[train_idx, df.columns != \"y\"], df.loc[train_idx, [\"y\"]],df.iloc[valid_idx, df.columns != \"y\"], df.loc[valid_idx, [\"y\"]], df.iloc[test_idx, df.columns != \"y\"], df.loc[test_idx, [\"y\"]],"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UijDo4pkCm7n"},"outputs":[],"source":["def select_features(data_tr_x, data_va_x, data_te_x, data_tr_y):\n","    # univariate feature selection\n","    first_trans = SelectPercentile(f_classif, percentile=70)\n","    first_trans.fit(data_tr_x, data_tr_y.values.ravel())\n","    data_tr_x_fs = first_trans.transform(data_tr_x)\n","    data_va_x_fs = first_trans.transform(data_va_x)\n","    data_te_x_fs = first_trans.transform(data_te_x)\n","\n","    # select from model\n","    clf = XGBClassifier(random_state=0)\n","    clf = clf.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","    second_trans = SelectFromModel(clf, prefit=True)\n","    data_tr_x_fs = second_trans.transform(data_tr_x_fs)\n","    data_va_x_fs = second_trans.transform(data_va_x_fs)\n","    data_te_x_fs = second_trans.transform(data_te_x_fs)\n","\n","    # get feature names\n","    mask1 = first_trans.get_support()\n","    mask2 = second_trans.get_support()\n","    new_feats1 = data_tr_x.columns[mask1] \n","    new_feats2 = new_feats1[mask2]\n","\n","    return data_tr_x_fs, data_va_x_fs, data_te_x_fs, new_feats2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ncPO1mvH-aG"},"outputs":[],"source":["# metrics\n","def statistical(y_true, y_pred, y_pro):\n","    c_mat = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = list(c_mat.flatten())\n","    se = tp / (tp + fn)\n","    sp = tn / (tn + fp)\n","    acc = (tp + tn) / (tn + fp + fn + tp)\n","    mcc = (tp * tn - fp * fn) / np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn) + 1e-8)\n","    auc_prc = auc(precision_recall_curve(y_true, y_pro, pos_label=1)[1],\n","                  precision_recall_curve(y_true, y_pro, pos_label=1)[0])\n","    auc_roc = roc_auc_score(y_true, y_pro)\n","    return tn, fp, fn, tp, se, sp, acc, mcc, auc_prc, auc_roc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bfam0ZLUHqN6"},"outputs":[],"source":["# calculate positive weight\n","def get_pos_weight(data):\n","    num_pos = F.sum(data.labels, dim=0)\n","    num_indices = F.tensor(len(data.labels))\n","    return (num_indices - num_pos) / num_pos"]},{"cell_type":"markdown","metadata":{"id":"L_AM_YNN_a_H"},"source":["# data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17581,"status":"ok","timestamp":1659002712245,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"RNGAdv5P_dJv","outputId":"b960e8e2-f18c-4e91-ae37-3590a5d5a105"},"outputs":[{"name":"stdout","output_type":"stream","text":["full descriptor data contains 233 features\n"]}],"source":["# get scaffold-split (full descriptors) data\n","data_tr_x, data_tr_y, data_va_x, data_va_y, data_te_x, data_te_y = get_split_data((\"./data/preprocessed data/1D_2D_PubChemFP_SubFP_preprocessed.csv\"))\n","print(f\"full descriptor data contains {data_tr_x.shape[1]} features\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23854,"status":"ok","timestamp":1659002736097,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"y8fOk2TvDIgt","outputId":"6e3a6de6-0e13-4937-9e26-67c577b1758e"},"outputs":[{"name":"stdout","output_type":"stream","text":["feature selection yielded 67 from 233 original features\n","selected features are: Index(['naAromAtom', 'nN', 'nO', 'nS', 'nP', 'AATS1m', 'AATS2m', 'AATS5m',\n","       'AATS7m', 'AATS3p', 'AATS4p', 'AATS2i', 'ATSC0m', 'ATSC4v', 'ATSC5v',\n","       'ATSC6v', 'ATSC0p', 'ATSC1p', 'ATSC3p', 'ATSC8p', 'ATSC3i', 'ATSC4i',\n","       'ATSC7i', 'AATSC1m', 'AATSC1v', 'AATSC8i', 'MATS6e', 'GATS1c', 'GATS2c',\n","       'GATS4c', 'GATS6c', 'GATS7c', 'GATS2m', 'GATS2e', 'GATS3e', 'GATS7i',\n","       'GATS3s', 'GATS4s', 'GATS5s', 'nBondsS3', 'nBondsD', 'C3SP2', 'C3SP3',\n","       'fragC', 'nHBAcc2', 'nHBDon', 'IC2', 'IC3', 'CIC2', 'MIC0', 'ZMIC1',\n","       'nAtomP', 'nAtomLAC', 'MLogP', 'MDEC-23', 'MDEO-22', 'MDEN-12',\n","       'MDEN-23', 'nRotB', 'topoRadius', 'GGI1', 'GGI2', 'GGI3', 'SpMAD_D',\n","       'EE_D', 'VE3_D', 'SRW5'],\n","      dtype='object')\n"]}],"source":["# get data post feature selection\n","data_tr_x_fs, data_va_x_fs, data_te_x_fs, new_feats = select_features(data_tr_x, data_va_x, data_te_x, data_tr_y)\n","print(f\"feature selection yielded {data_tr_x_fs.shape[1]} from {data_tr_x.shape[1]} original features\")\n","print(f\"selected features are: {new_feats}\")"]},{"cell_type":"markdown","metadata":{"id":"d_dfsASP1Ke1"},"source":["# experimentation overview"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1659002736097,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"QPvTWSbJ1O_o","outputId":"09880485-910c-4332-bb90-dded86094b24"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-13874511-57d3-44f7-8bfb-26dd24b1165c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>model_type</th>\n","      <th>filename</th>\n","      <th>GPU_accelerator</th>\n","      <th>RAM</th>\n","      <th>data_features</th>\n","      <th>hyperparameters</th>\n","      <th>train_performance_ROC-AUC_avg/std/max</th>\n","      <th>valid_performance_ROC-AUC_avg/std/max</th>\n","      <th>test_performance_ROC-AUC_avg/std/max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>SVM</td>\n","      <td>svm_opt_full_descr_5k_50</td>\n","      <td>Tesla P100-PCIE-16GB</td>\n","      <td>27.33 GB</td>\n","      <td>all features (233 feats)</td>\n","      <td>{'C': 40.52181430939162, 'break_ties': False, ...</td>\n","      <td>[0.99155, 1e-05, 0.99157]</td>\n","      <td>[0.79398, 7e-05, 0.79412]</td>\n","      <td>[0.65325, 4e-05, 0.65331]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>SVM</td>\n","      <td>svm_opt_feat_select_15k_50</td>\n","      <td>Tesla P100-PCIE-16GB</td>\n","      <td>27.33 GB</td>\n","      <td>feature selection (67 feats)</td>\n","      <td>{'C': 0.9146720748918769, 'break_ties': False,...</td>\n","      <td>[0.99813, 0.0, 0.99813]</td>\n","      <td>[0.79447, 0.0002, 0.79488]</td>\n","      <td>[0.75988, 0.00025, 0.76021]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>RF</td>\n","      <td>rf_opt_full_descr_50</td>\n","      <td>Tesla P100-PCIE-16GB</td>\n","      <td>27.33 GB</td>\n","      <td>all features (233 feats)</td>\n","      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n","      <td>[0.99116, 0.00023, 0.99144]</td>\n","      <td>[0.78926, 0.00851, 0.80319]</td>\n","      <td>[0.78592, 0.00485, 0.79239]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>RF</td>\n","      <td>rf_opt_feat_select_50</td>\n","      <td>Tesla P100-PCIE-16GB</td>\n","      <td>27.33 GB</td>\n","      <td>feature selection (67 feats)</td>\n","      <td>{'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...</td>\n","      <td>[0.95785, 0.00362, 0.96235]</td>\n","      <td>[0.74416, 0.04167, 0.81029]</td>\n","      <td>[0.73382, 0.01442, 0.76491]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>XGB</td>\n","      <td>xgb_opt_full_descr_50</td>\n","      <td>Tesla P100-PCIE-16GB</td>\n","      <td>27.33 GB</td>\n","      <td>all features (233 feats)</td>\n","      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n","      <td>[0.99891, 8e-05, 0.99908]</td>\n","      <td>[0.79425, 0.00737, 0.80282]</td>\n","      <td>[0.75037, 0.00928, 0.76298]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>XGB</td>\n","      <td>xgb_opt_feat_select_50</td>\n","      <td>Tesla P100-PCIE-16GB</td>\n","      <td>27.33 GB</td>\n","      <td>feature selection (67 feats)</td>\n","      <td>{'base_score': 0.5, 'booster': 'gbtree', 'cols...</td>\n","      <td>[0.89512, 0.0249, 0.92884]</td>\n","      <td>[0.7885, 0.00918, 0.80097]</td>\n","      <td>[0.76791, 0.01247, 0.77909]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>MLP</td>\n","      <td>mlp_opt_full_descr_50</td>\n","      <td>Tesla T4</td>\n","      <td>27.33 GB</td>\n","      <td>all features (233 feats)</td>\n","      <td>{'activation': 'relu', 'alpha': 0.000274680092...</td>\n","      <td>[0.9997, 0.00035, 0.99998]</td>\n","      <td>[0.7626, 0.01725, 0.79007]</td>\n","      <td>[0.70937, 0.01347, 0.73482]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13874511-57d3-44f7-8bfb-26dd24b1165c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-13874511-57d3-44f7-8bfb-26dd24b1165c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-13874511-57d3-44f7-8bfb-26dd24b1165c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  model_type                    filename        GPU_accelerator       RAM  \\\n","0        SVM    svm_opt_full_descr_5k_50  Tesla P100-PCIE-16GB   27.33 GB   \n","1        SVM  svm_opt_feat_select_15k_50  Tesla P100-PCIE-16GB   27.33 GB   \n","2         RF        rf_opt_full_descr_50  Tesla P100-PCIE-16GB   27.33 GB   \n","3         RF       rf_opt_feat_select_50  Tesla P100-PCIE-16GB   27.33 GB   \n","4        XGB       xgb_opt_full_descr_50  Tesla P100-PCIE-16GB   27.33 GB   \n","5        XGB      xgb_opt_feat_select_50  Tesla P100-PCIE-16GB   27.33 GB   \n","6        MLP       mlp_opt_full_descr_50              Tesla T4   27.33 GB   \n","\n","                  data_features  \\\n","0      all features (233 feats)   \n","1  feature selection (67 feats)   \n","2      all features (233 feats)   \n","3  feature selection (67 feats)   \n","4      all features (233 feats)   \n","5  feature selection (67 feats)   \n","6      all features (233 feats)   \n","\n","                                     hyperparameters  \\\n","0  {'C': 40.52181430939162, 'break_ties': False, ...   \n","1  {'C': 0.9146720748918769, 'break_ties': False,...   \n","2  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n","3  {'bootstrap': True, 'ccp_alpha': 0.0, 'class_w...   \n","4  {'base_score': 0.5, 'booster': 'gbtree', 'cols...   \n","5  {'base_score': 0.5, 'booster': 'gbtree', 'cols...   \n","6  {'activation': 'relu', 'alpha': 0.000274680092...   \n","\n","  train_performance_ROC-AUC_avg/std/max valid_performance_ROC-AUC_avg/std/max  \\\n","0             [0.99155, 1e-05, 0.99157]             [0.79398, 7e-05, 0.79412]   \n","1               [0.99813, 0.0, 0.99813]            [0.79447, 0.0002, 0.79488]   \n","2           [0.99116, 0.00023, 0.99144]           [0.78926, 0.00851, 0.80319]   \n","3           [0.95785, 0.00362, 0.96235]           [0.74416, 0.04167, 0.81029]   \n","4             [0.99891, 8e-05, 0.99908]           [0.79425, 0.00737, 0.80282]   \n","5            [0.89512, 0.0249, 0.92884]            [0.7885, 0.00918, 0.80097]   \n","6            [0.9997, 0.00035, 0.99998]            [0.7626, 0.01725, 0.79007]   \n","\n","  test_performance_ROC-AUC_avg/std/max  \n","0            [0.65325, 4e-05, 0.65331]  \n","1          [0.75988, 0.00025, 0.76021]  \n","2          [0.78592, 0.00485, 0.79239]  \n","3          [0.73382, 0.01442, 0.76491]  \n","4          [0.75037, 0.00928, 0.76298]  \n","5          [0.76791, 0.01247, 0.77909]  \n","6          [0.70937, 0.01347, 0.73482]  "]},"metadata":{},"output_type":"display_data"}],"source":["overview_df_filename = \"experimentation_overview\"\n","if not file_exists(overview_df_filename):\n","    print(\"no experiments conducted yet, please run the models\")\n","else:\n","    overview_df = pd.read_parquet(overview_df_filename)\n","    display(overview_df)"]},{"cell_type":"markdown","metadata":{"id":"6-COe-vu-Q7C"},"source":["# SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kfDZyXfqr2LA"},"outputs":[],"source":["# hyperparameter optimization setup\n","OPT_ITERS = 50\n","repetitions = 10\n","max_iter = 5000  \n","cache_size = 1000\n","\n","svm_hyper_space = {'C': hp.uniform('C', 0.1, 100),\n","                   'gamma': hp.uniform('gamma', 0, 0.3)}"]},{"cell_type":"markdown","metadata":{"id":"zBQlb_HF_Bi5"},"source":["## SVM (all features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrPuV4CRpRD6"},"outputs":[],"source":["def svm_hyper_opt(args):\n","    model = SVC(**args, kernel='rbf', random_state=0, probability=True, class_weight='balanced',\n","                    cache_size=cache_size, max_iter=max_iter, verbose =True) \n","    model.fit(data_tr_x, data_tr_y.values.ravel())\n","    val_preds = model.predict_proba(data_va_x)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9422394,"status":"ok","timestamp":1658839881171,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"LFtRRoOVTT1q","outputId":"f2ace64f-b716-447a-e30b-21d975ca1d81"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","[LibSVM]\n","  0%|          | 0/50 [00:00<?, ?it/s, best loss: ?]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  2%|▏         | 1/50 [04:44<3:52:01, 284.11s/it, best loss: 0.3271038482265335]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  4%|▍         | 2/50 [09:23<3:44:52, 281.10s/it, best loss: 0.3042098398001176]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  6%|▌         | 3/50 [14:01<3:39:08, 279.76s/it, best loss: 0.3042098398001176]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  8%|▊         | 4/50 [18:40<3:34:22, 279.63s/it, best loss: 0.2985621203213795]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 10%|█         | 5/50 [23:29<3:32:18, 283.08s/it, best loss: 0.2985621203213795]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 12%|█▏        | 6/50 [28:16<3:28:22, 284.15s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 14%|█▍        | 7/50 [32:53<3:22:07, 282.05s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 16%|█▌        | 8/50 [37:33<3:16:54, 281.31s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 18%|█▊        | 9/50 [42:01<3:09:15, 276.97s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 20%|██        | 10/50 [47:06<3:10:35, 285.88s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 22%|██▏       | 11/50 [51:46<3:04:35, 283.99s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 24%|██▍       | 12/50 [56:26<2:58:59, 282.63s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 26%|██▌       | 13/50 [1:01:07<2:54:05, 282.30s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 28%|██▊       | 14/50 [1:05:45<2:48:36, 281.02s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 30%|███       | 15/50 [1:09:41<2:36:00, 267.44s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 32%|███▏      | 16/50 [1:14:08<2:31:26, 267.24s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 34%|███▍      | 17/50 [1:18:36<2:27:03, 267.37s/it, best loss: 0.2590954463060945]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 36%|███▌      | 18/50 [1:21:22<2:06:23, 236.99s/it, best loss: 0.2300898368606703]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 38%|███▊      | 19/50 [1:26:17<2:11:26, 254.39s/it, best loss: 0.2300898368606703]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 40%|████      | 20/50 [1:30:45<2:09:13, 258.47s/it, best loss: 0.2300898368606703]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 42%|████▏     | 21/50 [1:33:38<1:52:30, 232.79s/it, best loss: 0.2300898368606703]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 44%|████▍     | 22/50 [1:36:34<1:40:47, 215.97s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 46%|████▌     | 23/50 [1:40:37<1:40:44, 223.88s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 48%|████▊     | 24/50 [1:44:31<1:38:23, 227.07s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 50%|█████     | 25/50 [1:49:03<1:40:12, 240.51s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 52%|█████▏    | 26/50 [1:52:55<1:35:10, 237.93s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 54%|█████▍    | 27/50 [1:57:24<1:34:46, 247.23s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 56%|█████▌    | 28/50 [2:02:15<1:35:29, 260.43s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 58%|█████▊    | 29/50 [2:07:03<1:34:02, 268.71s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 60%|██████    | 30/50 [2:10:34<1:23:45, 251.26s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 62%|██████▏   | 31/50 [2:15:31<1:23:58, 265.20s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 64%|██████▍   | 32/50 [2:19:32<1:17:19, 257.76s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 66%|██████▌   | 33/50 [2:22:37<1:06:50, 235.93s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 68%|██████▊   | 34/50 [2:27:01<1:05:07, 244.24s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 70%|███████   | 35/50 [2:31:30<1:02:59, 251.96s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 72%|███████▏  | 36/50 [2:36:03<1:00:11, 257.99s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 74%|███████▍  | 37/50 [2:40:39<57:05, 263.51s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 76%|███████▌  | 38/50 [2:45:15<53:27, 267.33s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 78%|███████▊  | 39/50 [2:49:48<49:20, 269.12s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 80%|████████  | 40/50 [2:53:25<42:14, 253.46s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 82%|████████▏ | 41/50 [2:57:49<38:28, 256.54s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 84%|████████▍ | 42/50 [3:02:16<34:37, 259.65s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 86%|████████▌ | 43/50 [3:06:31<30:08, 258.39s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 88%|████████▊ | 44/50 [3:11:05<26:17, 262.86s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 90%|█████████ | 45/50 [3:15:29<21:56, 263.26s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 92%|█████████▏| 46/50 [3:18:21<15:43, 235.79s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 94%|█████████▍| 47/50 [3:22:51<12:18, 246.18s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 96%|█████████▌| 48/50 [3:27:22<08:27, 253.74s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 98%|█████████▊| 49/50 [3:31:55<04:19, 259.40s/it, best loss: 0.20610731432490692]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["100%|██████████| 50/50 [3:36:28<00:00, 259.78s/it, best loss: 0.20610731432490692]\n","the best SVM hyperparameters are: {'C': 40.52181430939162, 'gamma': 0.003877592396436093}\n","[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["performing repetitions on different seeds\n","[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["model information added to experimentation overview\n","|    | model_type   | filename                 | GPU_accelerator      | RAM      | data_features            | hyperparameters                                                                                                                                                                                                                                                                                                             | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:-------------------------|:---------------------|:---------|:-------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50 | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats) | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True} | [0.99155, 1e-05, 0.99157]               | [0.79398, 7e-05, 0.79412]               | [0.65325, 4e-05, 0.65331]              |\n"]}],"source":["filename_svm_full_descr = \"svm_opt_full_descr_5k_50\" \n","\n","if file_exists(filename_svm_full_descr+\".sav\") and file_exists(filename_svm_full_descr+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best SVM model on full descriptors is:\")\n","    loaded_model = pickle.load(open(filename_svm_full_descr+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_svm_full_descr + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_svm_full_descr = fmin(svm_hyper_opt, svm_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    print(f\"the best SVM hyperparameters are: {best_results_svm_full_descr}\")\n","    best_model = SVC(C=best_results_svm_full_descr['C'], gamma=best_results_svm_full_descr['gamma'], kernel='rbf', random_state=0,\n","                            probability=True, class_weight='balanced', cache_size=cache_size, max_iter=max_iter, verbose=True)\n","    best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","    # save hyperparameters\n","    with open(filename_svm_full_descr+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_svm_full_descr, f)\n","    # loadable via ...\n","    # with open(filename_svm_full_descr+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_svm_full_descr+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_svm_full_descr+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = SVC(C=best_results_svm_full_descr['C'], gamma=best_results_svm_full_descr['gamma'], kernel='rbf', random_state=seed,\n","                            probability=True, class_weight='balanced', cache_size=cache_size, max_iter=max_iter, verbose=True)\n","        \n","        best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    svm_full_descr_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    svm_full_descr_perf.to_parquet(filename_svm_full_descr + \"_performance\", index=0)      \n","    # loadable via ...\n","    # svm_full_descr_perf = pd.read_parquet(filename_svm_full_descr + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(svm_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(svm_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(svm_full_descr_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"SVM\"\n","    data_features = f\"all features ({data_tr_x.shape[1]} feats)\"\n","    filename = filename_svm_full_descr\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hyperparameters = best_model.get_params() # all fixed and optimized hyperparameters\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"_rc7truq_Gdz"},"source":["## SVM (feature selection)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptEhDzHyhOf3"},"outputs":[],"source":["def svm_hyper_opt_fs(args):\n","    model = SVC(**args, kernel='rbf', random_state=0, probability=True, class_weight='balanced',\n","                    cache_size=cache_size, max_iter=max_iter, verbose =True) \n","    model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","    val_preds = model.predict_proba(data_va_x_fs)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6360834,"status":"ok","timestamp":1658846242004,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"3vRSCuZFhOf3","outputId":"9da3ffed-c278-4167-b984-cd486bee94cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","[LibSVM]\n","  0%|          | 0/50 [00:00<?, ?it/s, best loss: ?]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  2%|▏         | 1/50 [01:49<1:29:09, 109.18s/it, best loss: 0.24576535861258075]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  4%|▍         | 2/50 [03:02<1:10:28, 88.09s/it, best loss: 0.22471462864981384]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  6%|▌         | 3/50 [04:53<1:17:20, 98.72s/it, best loss: 0.22471462864981384]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n","  8%|▊         | 4/50 [06:50<1:21:00, 105.67s/it, best loss: 0.22471462864981384]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 10%|█         | 5/50 [08:09<1:12:05, 96.13s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 12%|█▏        | 6/50 [09:43<1:09:57, 95.41s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 14%|█▍        | 7/50 [10:53<1:02:32, 87.27s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 16%|█▌        | 8/50 [12:30<1:03:12, 90.29s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 18%|█▊        | 9/50 [13:39<57:11, 83.70s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 20%|██        | 10/50 [15:26<1:00:26, 90.67s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 22%|██▏       | 11/50 [17:11<1:01:50, 95.13s/it, best loss: 0.21772578630217532]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 24%|██▍       | 12/50 [19:08<1:04:28, 101.79s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 26%|██▌       | 13/50 [21:00<1:04:41, 104.90s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 28%|██▊       | 14/50 [22:46<1:03:04, 105.11s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 30%|███       | 15/50 [24:40<1:02:59, 107.97s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 32%|███▏      | 16/50 [26:33<1:02:05, 109.56s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 34%|███▍      | 17/50 [27:47<54:22, 98.87s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 36%|███▌      | 18/50 [29:38<54:31, 102.24s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 38%|███▊      | 19/50 [31:29<54:12, 104.93s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 40%|████      | 20/50 [33:08<51:36, 103.23s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 42%|████▏     | 21/50 [35:03<51:39, 106.86s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 44%|████▍     | 22/50 [36:45<49:08, 105.31s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 46%|████▌     | 23/50 [38:26<46:51, 104.14s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 48%|████▊     | 24/50 [39:56<43:10, 99.65s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 50%|█████     | 25/50 [41:24<40:09, 96.36s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 52%|█████▏    | 26/50 [43:03<38:47, 96.99s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 54%|█████▍    | 27/50 [44:37<36:50, 96.11s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 56%|█████▌    | 28/50 [46:32<37:23, 101.96s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 58%|█████▊    | 29/50 [47:47<32:48, 93.72s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 60%|██████    | 30/50 [49:20<31:07, 93.39s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 62%|██████▏   | 31/50 [50:58<30:00, 94.79s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 64%|██████▍   | 32/50 [52:41<29:13, 97.42s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 66%|██████▌   | 33/50 [54:29<28:28, 100.52s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 68%|██████▊   | 34/50 [56:18<27:26, 102.93s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 70%|███████   | 35/50 [57:49<24:51, 99.44s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 72%|███████▏  | 36/50 [59:33<23:33, 100.96s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 74%|███████▍  | 37/50 [1:00:43<19:50, 91.61s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 76%|███████▌  | 38/50 [1:01:47<16:39, 83.27s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 78%|███████▊  | 39/50 [1:03:26<16:09, 88.12s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 80%|████████  | 40/50 [1:04:52<14:34, 87.41s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 82%|████████▏ | 41/50 [1:06:44<14:12, 94.68s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 84%|████████▍ | 42/50 [1:08:27<12:57, 97.22s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 86%|████████▌ | 43/50 [1:10:15<11:43, 100.50s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 88%|████████▊ | 44/50 [1:11:57<10:04, 100.80s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 90%|█████████ | 45/50 [1:13:40<08:28, 101.66s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 92%|█████████▏| 46/50 [1:15:02<06:22, 95.63s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 94%|█████████▍| 47/50 [1:16:12<04:23, 87.94s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 96%|█████████▌| 48/50 [1:17:50<03:02, 91.06s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]\n"," 98%|█████████▊| 49/50 [1:19:03<01:25, 85.63s/it, best loss: 0.20573375955320405]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["100%|██████████| 50/50 [1:20:51<00:00, 97.03s/it, best loss: 0.20573375955320405]\n","the best SVM hyper-parameters are: {'C': 0.9146720748918769, 'gamma': 0.07830765964249707}\n","[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["performing repetitions on different seeds\n","[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["[LibSVM]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:289: ConvergenceWarning: Solver terminated early (max_iter=5000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  ConvergenceWarning,\n"]},{"name":"stdout","output_type":"stream","text":["model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                             | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True} | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True} | [0.99813, 0.0, 0.99813]                 | [0.79447, 0.0002, 0.79488]              | [0.75988, 0.00025, 0.76021]            |\n"]}],"source":["filename_svm_feat_select = \"svm_opt_feat_select_5k_50\"\n","\n","if file_exists(filename_svm_feat_select+\".sav\") and file_exists(filename_svm_feat_select+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best SVM model with feature selection is:\")\n","    loaded_model = pickle.load(open(filename_svm_feat_select+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_svm_feat_select + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_svm_feat_select = fmin(svm_hyper_opt_fs, svm_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    print(f\"the best SVM hyper-parameters are: {best_results_svm_feat_select}\")\n","    best_model = SVC(C=best_results_svm_feat_select['C'], gamma=best_results_svm_feat_select['gamma'], kernel='rbf', random_state=0,\n","                            probability=True, class_weight='balanced', cache_size=2000, max_iter=max_iter, verbose=True)\n","    best_model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","\n","    # save hyperparameters\n","    with open(filename_svm_feat_select+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_svm_feat_select, f)\n","    # loadable via ...\n","    # with open(filename_svm_feat_select+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_svm_feat_select+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_svm_feat_select+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = SVC(C=best_results_svm_feat_select['C'], gamma=best_results_svm_feat_select['gamma'], kernel='rbf', random_state=seed,\n","                            probability=True, class_weight='balanced', cache_size=cache_size, max_iter=max_iter, verbose=True)\n","        \n","        best_model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x_fs)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x_fs)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x_fs)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    svm_feat_select_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    svm_feat_select_perf.to_parquet(filename_svm_feat_select + \"_performance\", index=0)      \n","    # loadable via ...\n","    # svm_feat_select_perf = pd.read_parquet(filename_svm_feat_select + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(svm_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(svm_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(svm_feat_select_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"SVM\"\n","    data_features = f\"feature selection ({data_tr_x_fs.shape[1]} feats)\"\n","    filename = filename_svm_feat_select\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hyperparameters = best_model.get_params() # all fixed and optimized hyperparameters\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        # save\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"ti-jgCrC-WO7"},"source":["# RF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3GD1n7-_wgkH"},"outputs":[],"source":["# hyperparameter optimization setup for RF\n","OPT_ITERS = 50 \n","repetitions = 10 \n","\n","rf_hyper_space = {'n_estimators': hp.choice('n_estimators', [10, 25, 50, 100, 200, 300, 400, 500]),\n","                  'max_depth': hp.choice('max_depth', range(3, 15)),\n","                  'min_samples_leaf': hp.choice('min_samples_leaf', [1, 3, 5, 10, 25, 50]),\n","                  'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 0.02),\n","                  'max_features': hp.choice('max_features', ['sqrt', 'log2', 0.2, 0.4, 0.6, 0.8])}\n","\n","# hyperparameter lists for building best model (for all hyperparameters with hp.choice())\n","n_estimators_ls = [10, 25, 50, 100, 200, 300, 400, 500]\n","max_depth_ls = range(3, 15)\n","min_samples_leaf_ls = [1, 3, 5, 10, 25, 50]\n","max_features_ls = ['sqrt', 'log2', 0.2, 0.4, 0.6, 0.8]"]},{"cell_type":"markdown","metadata":{"id":"O0Pdrn99_JNx"},"source":["## RF (all features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mFQJ22Ywgb2"},"outputs":[],"source":["def rf_hyper_opt(args):\n","    model = RandomForestClassifier(**args, n_jobs=-1, random_state=0, verbose=0, class_weight='balanced')\n","    model.fit(data_tr_x, data_tr_y.values.ravel())\n","    val_preds = model.predict_proba(data_va_x)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3632343,"status":"ok","timestamp":1658849874346,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"s2tWPQQFpNah","outputId":"2b04f43a-411a-472f-b972-621d39f3016a"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","100%|██████████| 50/50 [49:28<00:00, 59.38s/it, best loss: 0.20491928767391732]\n","the best RF hyperparameters are: n_estimators 100 | max_depth 13 | min_samples_leaf 3 | max_features 0.4 | min_impurity_decrease 0.0007866053743963098\n","performing repetitions on different seeds\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                             | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                 | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                 | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False} | [0.99116, 0.00023, 0.99144]             | [0.78926, 0.00851, 0.80319]             | [0.78592, 0.00485, 0.79239]            |\n"]}],"source":["filename_rf_full_descr = \"rf_opt_full_descr_50\"\n","\n","if file_exists(filename_rf_full_descr+\".sav\") and file_exists(filename_rf_full_descr+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best RF model on full descriptors is:\")\n","    loaded_model = pickle.load(open(filename_rf_full_descr+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_rf_full_descr + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_rf_full_descr = fmin(rf_hyper_opt, rf_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    \n","    text = (\n","            \"the best RF hyperparameters are: \"\n","            f\"n_estimators {n_estimators_ls[best_results_rf_full_descr['n_estimators']]} | \"\n","            f\"max_depth {max_depth_ls[best_results_rf_full_descr['max_depth']]} | \"\n","            f\"min_samples_leaf {min_samples_leaf_ls[best_results_rf_full_descr['min_samples_leaf']]} | \"\n","            f\"max_features {max_features_ls[best_results_rf_full_descr['max_features']]} | \"\n","            f\"min_impurity_decrease {best_results_rf_full_descr['min_impurity_decrease']}\"\n","            )\n","    print(text)\n","    best_model = RandomForestClassifier(n_estimators=n_estimators_ls[best_results_rf_full_descr['n_estimators']],\n","                                        max_depth=max_depth_ls[best_results_rf_full_descr['max_depth']],\n","                                        min_samples_leaf=min_samples_leaf_ls[best_results_rf_full_descr['min_samples_leaf']],\n","                                        max_features=max_features_ls[best_results_rf_full_descr['max_features']],\n","                                        min_impurity_decrease=best_results_rf_full_descr['min_impurity_decrease'],\n","                                        n_jobs=-1, random_state=0, verbose=0, class_weight='balanced')\n","\n","    best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","    # save hyperparameters\n","    with open(filename_rf_full_descr+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_rf_full_descr, f)\n","    # loadable via ...\n","    # with open(filename_rf_full_descr+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_rf_full_descr+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_rf_full_descr+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = RandomForestClassifier(n_estimators=n_estimators_ls[best_results_rf_full_descr['n_estimators']],\n","                                            max_depth=max_depth_ls[best_results_rf_full_descr['max_depth']],\n","                                            min_samples_leaf=min_samples_leaf_ls[best_results_rf_full_descr['min_samples_leaf']],\n","                                            max_features=max_features_ls[best_results_rf_full_descr['max_features']],\n","                                            min_impurity_decrease=best_results_rf_full_descr['min_impurity_decrease'],\n","                                            n_jobs=-1, random_state=seed, verbose=0, class_weight='balanced')\n","        \n","        best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    rf_full_descr_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    rf_full_descr_perf.to_parquet(filename_rf_full_descr + \"_performance\", index=0)      \n","    # loadable via ...\n","    # rf_full_descr_perf = pd.read_parquet(filename_rf_full_descr + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(rf_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(rf_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(rf_full_descr_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"RF\"\n","    data_features = f\"all features ({data_tr_x.shape[1]} feats)\"\n","    filename = filename_rf_full_descr\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hp_dict = best_model.get_params() # all fixed and optimized hyperparameters\n","    hyperparameters = hp_dict\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        overview_df[\"hyperparameters\"] = overview_df[\"hyperparameters\"].astype(str)\n","        # save new df\n","        overview_df.to_parquet(overview_df_filename, index=0)   \n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"RPHd4OLN_Moo"},"source":["## RF (feature selection)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxanx4GipWtr"},"outputs":[],"source":["def rf_hyper_opt_fs(args):\n","    model = RandomForestClassifier(**args, n_jobs=-1, random_state=0, verbose=0, class_weight='balanced')\n","    model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","    val_preds = model.predict_proba(data_va_x_fs)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":882096,"status":"ok","timestamp":1658852970014,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"hhWIZYiNbJGH","outputId":"4a7e3bc7-9f27-4b19-e63a-6452db823633"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","100%|██████████| 50/50 [14:33<00:00, 17.46s/it, best loss: 0.18970917842445623]\n","the best RF hyperparameters are: n_estimators 10 | max_depth 10 | min_samples_leaf 3 | max_features log2 | min_impurity_decrease 0.0003677058219403621\n","performing repetitions on different seeds\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                               | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                   | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                   | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}   | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False} | [0.95785, 0.00362, 0.96235]             | [0.74416, 0.04167, 0.81029]             | [0.73382, 0.01442, 0.76491]            |\n"]}],"source":["filename_rf_feat_select = \"rf_opt_feat_select_50\"\n","\n","if file_exists(filename_rf_feat_select+\".sav\") and file_exists(filename_rf_feat_select+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best RF model on feature selection is:\")\n","    loaded_model = pickle.load(open(filename_rf_feat_select+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_rf_feat_select + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_rf_feat_select = fmin(rf_hyper_opt_fs, rf_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    \n","    text = (\n","            \"the best RF hyperparameters are: \"\n","            f\"n_estimators {n_estimators_ls[best_results_rf_feat_select['n_estimators']]} | \"\n","            f\"max_depth {max_depth_ls[best_results_rf_feat_select['max_depth']]} | \"\n","            f\"min_samples_leaf {min_samples_leaf_ls[best_results_rf_feat_select['min_samples_leaf']]} | \"\n","            f\"max_features {max_features_ls[best_results_rf_feat_select['max_features']]} | \"\n","            f\"min_impurity_decrease {best_results_rf_feat_select['min_impurity_decrease']}\"\n","            )\n","    print(text)\n","    best_model = RandomForestClassifier(n_estimators=n_estimators_ls[best_results_rf_feat_select['n_estimators']],\n","                                        max_depth=max_depth_ls[best_results_rf_feat_select['max_depth']],\n","                                        min_samples_leaf=min_samples_leaf_ls[best_results_rf_feat_select['min_samples_leaf']],\n","                                        max_features=max_features_ls[best_results_rf_feat_select['max_features']],\n","                                        min_impurity_decrease=best_results_rf_feat_select['min_impurity_decrease'],\n","                                        n_jobs=-1, random_state=0, verbose=0, class_weight='balanced')\n","\n","    best_model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","\n","    # save hyperparameters\n","    with open(filename_rf_feat_select+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_rf_feat_select, f)\n","    # loadable via ...\n","    # with open(filename_rf_feat_select+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_rf_feat_select+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_rf_feat_select+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = RandomForestClassifier(n_estimators=n_estimators_ls[best_results_rf_feat_select['n_estimators']],\n","                                            max_depth=max_depth_ls[best_results_rf_feat_select['max_depth']],\n","                                            min_samples_leaf=min_samples_leaf_ls[best_results_rf_feat_select['min_samples_leaf']],\n","                                            max_features=max_features_ls[best_results_rf_feat_select['max_features']],\n","                                            min_impurity_decrease=best_results_rf_feat_select['min_impurity_decrease'],\n","                                            n_jobs=-1, random_state=seed, verbose=0, class_weight='balanced')\n","        \n","        best_model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x_fs)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x_fs)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x_fs)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    rf_feat_select_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    rf_feat_select_perf.to_parquet(filename_rf_feat_select + \"_performance\", index=0)      \n","    # loadable via ...\n","    # rf_feat_select_perf = pd.read_parquet(filename_rf_feat_select + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(rf_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(rf_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(rf_feat_select_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"RF\"\n","    data_features = f\"feature selection ({data_tr_x_fs.shape[1]} feats)\"\n","    filename = filename_rf_feat_select\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hp_dict = best_model.get_params() # all fixed and optimized hyperparameters\n","    hyperparameters = hp_dict\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        overview_df[\"hyperparameters\"] = overview_df[\"hyperparameters\"].astype(str)\n","        # save new df\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"ZyKM3eRi-TbT"},"source":["# XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrB4jurkFywv"},"outputs":[],"source":["# hyperparameter optimization setup for XGB\n","OPT_ITERS = 50\n","repetitions = 10\n","patience = 25\n","\n","xgb_hyper_space = {'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n","                   'gamma': hp.uniform('gamma', 0, 0.3),\n","                   'min_child_weight': hp.choice('min_child_weight', range(1, 5)),\n","                   'subsample': hp.uniform('subsample', 0.5, 1.0),\n","                   'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n","                   'max_depth': hp.choice('max_depth', range(3, 12)),\n","                   'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500, 1000, 1500, 2000])}\n","\n","# hyperparameter lists for building best model (for all hyperparameters with hp.choice())\n","min_child_weight_ls = range(1, 5)\n","max_depth_ls = range(3, 12)\n","n_estimators_ls = [100, 200, 300, 400, 500, 1000, 1500, 2000]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"stsmcZSSFbLn"},"outputs":[],"source":["# calculate weight for positive data instances\n","dataset = DglGraphPropPredDataset(name = \"ogbg-molhiv\")\n","pos_weight = float(get_pos_weight(dataset))"]},{"cell_type":"markdown","metadata":{"id":"Mc1aPbFf-ZPy"},"source":["## XGBoost (all features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TZ-eUyN_PUj"},"outputs":[],"source":["def xgb_hyper_opt(args):\n","    model = XGBClassifier(**args, n_jobs=-1, random_state=0, scale_pos_weight=pos_weight)\n","    model.fit(data_tr_x, data_tr_y.values.ravel(),\n","              eval_metric='auc', eval_set=[(data_va_x, data_va_y.values.ravel())],\n","              early_stopping_rounds=patience, verbose=False)\n","    val_preds = model.predict_proba(data_va_x)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4131262,"status":"ok","timestamp":1658857108378,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"JSbHvmvoEun-","outputId":"ff8ef34e-4c3d-46fe-e1ac-5b21fbffda4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","100%|██████████| 50/50 [42:03<00:00, 50.46s/it, best loss: 0.18452074759945136]\n","the best XGB hyperparameters are: learning_rate 0.20941648886207803 | gamma 0.22369342452726626 | min_child_weight 1 | subsample 0.910169049816008 | colsample_bytree 0.9436828391115852 | max_depth 3 | n_estimators 500\n","performing repetitions on different seeds\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [0.99891, 8e-05, 0.99908]               | [0.79425, 0.00737, 0.80282]             | [0.75037, 0.00928, 0.76298]            |\n"]}],"source":["filename_xgb_full_descr = \"xgb_opt_full_descr_50\"\n","\n","if file_exists(filename_xgb_full_descr+\".sav\") and file_exists(filename_xgb_full_descr+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best XGB model on full descriptors is:\")\n","    loaded_model = pickle.load(open(filename_xgb_full_descr+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_xgb_full_descr + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_xgb_full_descr = fmin(xgb_hyper_opt, xgb_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    \n","    text = (\n","            \"the best XGB hyperparameters are: \"\n","            f\"learning_rate {best_results_xgb_full_descr['learning_rate']} | \"\n","            f\"gamma {best_results_xgb_full_descr['gamma']} | \"\n","            f\"min_child_weight {min_child_weight_ls[best_results_xgb_full_descr['min_child_weight']]} | \"\n","            f\"subsample {best_results_xgb_full_descr['subsample']} | \"\n","            f\"colsample_bytree {best_results_xgb_full_descr['colsample_bytree']} | \"\n","            f\"max_depth {max_depth_ls[best_results_xgb_full_descr['max_depth']]} | \"\n","            f\"n_estimators {n_estimators_ls[best_results_xgb_full_descr['n_estimators']]}\"\n","            )\n","    print(text)\n","    best_model = XGBClassifier(learning_rate = best_results_xgb_full_descr['learning_rate'],\n","                               gamma = best_results_xgb_full_descr['gamma'],\n","                               min_child_weight = min_child_weight_ls[best_results_xgb_full_descr['min_child_weight']],\n","                               subsample = best_results_xgb_full_descr['subsample'],\n","                               colsample_bytree = best_results_xgb_full_descr['colsample_bytree'],\n","                               max_depth = max_depth_ls[best_results_xgb_full_descr['max_depth']],\n","                               n_estimators = n_estimators_ls[best_results_xgb_full_descr['n_estimators']],\n","                               n_jobs=-1, random_state=0, scale_pos_weight=pos_weight)\n","\n","    best_model.fit(data_tr_x, data_tr_y.values.ravel(),\n","              eval_metric='auc', eval_set=[(data_va_x, data_va_y.values.ravel())],\n","              early_stopping_rounds=patience, verbose=False)\n","\n","    # save hyperparameters\n","    with open(filename_xgb_full_descr+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_xgb_full_descr, f)\n","    # loadable via ...\n","    # with open(filename_xgb_full_descr+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_xgb_full_descr+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_xgb_full_descr+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = XGBClassifier(learning_rate = best_results_xgb_full_descr['learning_rate'],\n","                               gamma = best_results_xgb_full_descr['gamma'],\n","                               min_child_weight = min_child_weight_ls[best_results_xgb_full_descr['min_child_weight']],\n","                               subsample = best_results_xgb_full_descr['subsample'],\n","                               colsample_bytree = best_results_xgb_full_descr['colsample_bytree'],\n","                               max_depth = max_depth_ls[best_results_xgb_full_descr['max_depth']],\n","                               n_estimators = n_estimators_ls[best_results_xgb_full_descr['n_estimators']],\n","                               n_jobs=-1, random_state=seed, scale_pos_weight=pos_weight)\n","        \n","        best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    xgb_full_descr_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    xgb_full_descr_perf.to_parquet(filename_xgb_full_descr + \"_performance\", index=0)      \n","    # loadable via ...\n","    # xgb_full_descr_perf = pd.read_parquet(filename_xgb_full_descr + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(xgb_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(xgb_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(xgb_full_descr_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"XGB\"\n","    data_features = f\"all features ({data_tr_x.shape[1]} feats)\"\n","    filename = filename_xgb_full_descr\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hp_dict = best_model.get_params() # all fixed and optimized hyperparameters\n","    hyperparameters = hp_dict\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        overview_df[\"hyperparameters\"] = overview_df[\"hyperparameters\"].astype(str)\n","        # save new df\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"TDuMa4RU_Pah"},"source":["## XGBoost (feature selection)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrHubeuE-ZTS"},"outputs":[],"source":["def xgb_hyper_opt_fs(args):\n","    model = XGBClassifier(**args, n_jobs=-1, random_state=0, scale_pos_weight=pos_weight)\n","    model.fit(data_tr_x_fs, data_tr_y.values.ravel(),\n","              eval_metric='auc', eval_set=[(data_va_x_fs, data_va_y.values.ravel())],\n","              early_stopping_rounds=patience, verbose=False)\n","    val_preds = model.predict_proba(data_va_x_fs)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":871653,"status":"ok","timestamp":1658857980028,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"QJzfRiJiFGdA","outputId":"85e281ab-7056-42dc-f019-4257a23de620"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","100%|██████████| 50/50 [13:08<00:00, 15.76s/it, best loss: 0.19903273809523814]\n","the best XGB hyperparameters are: learning_rate 0.1246187700498615 | gamma 0.1982639800431708 | min_child_weight 4 | subsample 0.7215172738554442 | colsample_bytree 0.7144328026415806 | max_depth 3 | n_estimators 300\n","performing repetitions on different seeds\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [9.9891e-01 8.0000e-05 9.9908e-01]      | [0.79425 0.00737 0.80282]               | [0.75037 0.00928 0.76298]              |\n","|  5 | XGB          | xgb_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7144328026415806, 'gamma': 0.1982639800431708, 'learning_rate': 0.1246187700498615, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 4, 'missing': None, 'n_estimators': 300, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 881799, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.7215172738554442, 'verbosity': 1}  | [0.89512, 0.0249, 0.92884]              | [0.7885, 0.00918, 0.80097]              | [0.76791, 0.01247, 0.77909]            |\n"]}],"source":["filename_xgb_feat_select = \"xgb_opt_feat_select_50\"\n","\n","if file_exists(filename_xgb_feat_select+\".sav\") and file_exists(filename_xgb_feat_select+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best XGB model on feature selection is:\")\n","    loaded_model = pickle.load(open(filename_xgb_feat_select+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_xgb_feat_select + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_xgb_feat_select = fmin(xgb_hyper_opt_fs, xgb_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    \n","    text = (\n","            \"the best XGB hyperparameters are: \"\n","            f\"learning_rate {best_results_xgb_feat_select['learning_rate']} | \"\n","            f\"gamma {best_results_xgb_feat_select['gamma']} | \"\n","            f\"min_child_weight {min_child_weight_ls[best_results_xgb_feat_select['min_child_weight']]} | \"\n","            f\"subsample {best_results_xgb_feat_select['subsample']} | \"\n","            f\"colsample_bytree {best_results_xgb_feat_select['colsample_bytree']} | \"\n","            f\"max_depth {max_depth_ls[best_results_xgb_feat_select['max_depth']]} | \"\n","            f\"n_estimators {n_estimators_ls[best_results_xgb_feat_select['n_estimators']]}\"\n","            )\n","    print(text)\n","    best_model = XGBClassifier(learning_rate = best_results_xgb_feat_select['learning_rate'],\n","                               gamma = best_results_xgb_feat_select['gamma'],\n","                               min_child_weight = min_child_weight_ls[best_results_xgb_feat_select['min_child_weight']],\n","                               subsample = best_results_xgb_full_descr['subsample'],\n","                               colsample_bytree = best_results_xgb_feat_select['colsample_bytree'],\n","                               max_depth = max_depth_ls[best_results_xgb_feat_select['max_depth']],\n","                               n_estimators = n_estimators_ls[best_results_xgb_feat_select['n_estimators']],\n","                               n_jobs=-1, random_state=0, scale_pos_weight=pos_weight)\n","\n","    best_model.fit(data_tr_x_fs, data_tr_y.values.ravel(),\n","              eval_metric='auc', eval_set=[(data_va_x_fs, data_va_y.values.ravel())],\n","              early_stopping_rounds=patience, verbose=False)\n","    \n","    # save hyperparameters\n","    with open(filename_xgb_feat_select+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_xgb_feat_select, f)\n","    # loadable via ...\n","    # with open(filename_xgb_feat_select+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_xgb_feat_select+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_xgb_feat_select+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = XGBClassifier(learning_rate = best_results_xgb_feat_select['learning_rate'],\n","                               gamma = best_results_xgb_feat_select['gamma'],\n","                               min_child_weight = min_child_weight_ls[best_results_xgb_feat_select['min_child_weight']],\n","                               subsample = best_results_xgb_feat_select['subsample'],\n","                               colsample_bytree = best_results_xgb_feat_select['colsample_bytree'],\n","                               max_depth = max_depth_ls[best_results_xgb_feat_select['max_depth']],\n","                               n_estimators = n_estimators_ls[best_results_xgb_feat_select['n_estimators']],\n","                               n_jobs=-1, random_state=seed, scale_pos_weight=pos_weight)\n","            \n","        best_model.fit(data_tr_x_fs, data_tr_y.values.ravel(),\n","                eval_metric='auc', eval_set=[(data_va_x_fs, data_va_y.values.ravel())],\n","                early_stopping_rounds=patience, verbose=False)\n","        \n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x_fs)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x_fs)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x_fs)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    xgb_feat_select_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    xgb_feat_select_perf.to_parquet(filename_xgb_feat_select + \"_performance\", index=0)      \n","    # loadable via ...\n","    # xgb_feat_select_perf = pd.read_parquet(filename_xgb_feat_select + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(xgb_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(xgb_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(xgb_feat_select_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"XGB\"\n","    data_features = f\"feature selection ({data_tr_x_fs.shape[1]} feats)\"\n","    filename = filename_xgb_feat_select\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hp_dict = best_model.get_params() # all fixed and optimized hyperparameters\n","    hyperparameters = hp_dict\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        overview_df[\"hyperparameters\"] = overview_df[\"hyperparameters\"].astype(str)\n","        # save new df\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"bscpfHni-ZW2"},"source":["# MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYFQl295cxmt"},"outputs":[],"source":["# hyperparameter optimization setup for MLP\n","OPT_ITERS =50\n","repetitions =10\n","patience_mlp =50\n","epochs = 300 \n","max_iter_mlp = epochs\n","batch_size = 128\n","\n","mlp_hyper_space = {'learning_rate_init': hp.choice('learning_rate_init', [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]),\n","                   'alpha': hp.uniform('alpha', 0, 0.01), # l2 regularization\n","                   'hidden_layer_sizes': hp.choice('hidden_layer_sizes', [(50, 50), (100, 100), (100, 100, 100), (250, 250, 250), (500, 500, 500)])}\n","# keep activation default (relu)\n","# keep solver default at adam\n","\n","# hyperparameter lists for building best model (for all hyperparameters with hp.choice())\n","learning_rate_init_ls = [10 ** -2.5, 10 ** -3.5, 10 ** -1.5]\n","hidden_layer_sizes_ls = [(50, 50), (100, 100), (100, 100, 100), (250, 250, 250), (500, 500, 500)]"]},{"cell_type":"markdown","metadata":{"id":"PUrdCqbl-cD8"},"source":["## MLP (all features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ui1vzODqcpIU"},"outputs":[],"source":["def mlp_hyper_opt(args):\n","    model = MLPClassifier(**args, max_iter=max_iter_mlp, random_state=0, n_iter_no_change=patience_mlp)\n","    model.fit(data_tr_x, data_tr_y.values.ravel())\n","    val_preds = model.predict_proba(data_va_x)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29097448,"status":"ok","timestamp":1658941864888,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"gpZFxGsI_T0X","outputId":"7287882b-5a52-470e-fe2d-231faa165084"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","  2%|▏         | 1/50 [03:05<2:31:22, 185.36s/it, best loss: 0.2796639231824417]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r  4%|▍         | 2/50 [04:27<1:39:50, 124.80s/it, best loss: 0.2749393738977074]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 18%|█▊        | 9/50 [57:41<5:33:41, 488.32s/it, best loss: 0.23312573486184596]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 24%|██▍       | 12/50 [1:11:29<3:50:36, 364.12s/it, best loss: 0.23295273613560652]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r 26%|██▌       | 13/50 [1:15:38<3:23:08, 329.42s/it, best loss: 0.2188326719576721] "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 38%|███▊      | 19/50 [1:44:41<2:05:49, 243.52s/it, best loss: 0.20994390554575737]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 68%|██████▊   | 34/50 [4:02:21<1:21:00, 303.75s/it, best loss: 0.20994390554575737]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 74%|███████▍  | 37/50 [4:08:44<42:56, 198.21s/it, best loss: 0.20994390554575737]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 92%|█████████▏| 46/50 [6:55:17<1:05:44, 986.09s/it, best loss: 0.20994390554575737] "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 96%|█████████▌| 48/50 [7:29:22<30:47, 923.93s/it, best loss: 0.20994390554575737]   "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["100%|██████████| 50/50 [7:32:16<00:00, 542.74s/it, best loss: 0.20994390554575737]\n","the best MLP hyperparameters are: learning_rate_init 0.0031622776601683794 | l2 0.0002746800923919801 | hidden_layer_sizes (100, 100, 100)\n","performing repetitions on different seeds\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [9.9891e-01 8.0000e-05 9.9908e-01]      | [0.79425 0.00737 0.80282]               | [0.75037 0.00928 0.76298]              |\n","|  5 | XGB          | xgb_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7144328026415806, 'gamma': 0.1982639800431708, 'learning_rate': 0.1246187700498615, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 4, 'missing': None, 'n_estimators': 300, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 881799, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.7215172738554442, 'verbosity': 1}  | [0.89512 0.0249  0.92884]               | [0.7885  0.00918 0.80097]               | [0.76791 0.01247 0.77909]              |\n","|  6 | MLP          | mlp_opt_full_descr_50      | Tesla T4             | 27.33 GB | all features (233 feats)     | {'activation': 'relu', 'alpha': 0.0002746800923919801, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.0031622776601683794, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 353006, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}       | [0.9997, 0.00035, 0.99998]              | [0.7626, 0.01725, 0.79007]              | [0.70937, 0.01347, 0.73482]            |\n"]}],"source":["filename_mlp_full_descr = \"mlp_opt_full_descr_50\"\n","\n","if file_exists(filename_mlp_full_descr+\".sav\") and file_exists(filename_mlp_full_descr+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best MLP model on full descriptors is:\")\n","    loaded_model = pickle.load(open(filename_mlp_full_descr+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_mlp_full_descr + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_mlp_full_descr = fmin(mlp_hyper_opt, mlp_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    \n","    text = (\n","            \"the best MLP hyperparameters are: \"\n","            f\"learning_rate_init {learning_rate_init_ls[best_results_mlp_full_descr['learning_rate_init']]} | \"\n","            f\"l2 {best_results_mlp_full_descr['alpha']} | \"\n","            f\"hidden_layer_sizes {hidden_layer_sizes_ls[best_results_mlp_full_descr['hidden_layer_sizes']]}\"\n","            )\n","    print(text)\n","    best_model = MLPClassifier(learning_rate_init= learning_rate_init_ls[best_results_mlp_full_descr['learning_rate_init']],\n","                               alpha=best_results_mlp_full_descr['alpha'],\n","                               hidden_layer_sizes = hidden_layer_sizes_ls[best_results_mlp_full_descr['hidden_layer_sizes']],\n","                               max_iter=max_iter_mlp, random_state=0, n_iter_no_change=patience_mlp)\n","\n","    # best_model.fit(data_tr_x.to_numpy().astype(float), data_tr_y.values.ravel().astype(int))\n","    best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","    # save hyperparameters\n","    with open(filename_mlp_full_descr+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_mlp_full_descr, f)\n","    # loadable via ...\n","    # with open(filename_mlp_full_descr+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_mlp_full_descr+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_mlp_full_descr+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = MLPClassifier(learning_rate_init= learning_rate_init_ls[best_results_mlp_full_descr['learning_rate_init']],\n","                               alpha=best_results_mlp_full_descr['alpha'],\n","                               hidden_layer_sizes = hidden_layer_sizes_ls[best_results_mlp_full_descr['hidden_layer_sizes']],\n","                               max_iter=max_iter_mlp, random_state=seed, n_iter_no_change=patience_mlp)\n","        \n","        best_model.fit(data_tr_x, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    mlp_full_descr_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    mlp_full_descr_perf.to_parquet(filename_mlp_full_descr + \"_performance\", index=0)      \n","    # loadable via ...\n","    # mlp_full_descr_perf = pd.read_parquet(filename_mlp_full_descr + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(mlp_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(mlp_full_descr_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(mlp_full_descr_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"MLP\"\n","    data_features = f\"all features ({data_tr_x.shape[1]} feats)\"\n","    filename = filename_mlp_full_descr\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hp_dict = best_model.get_params() # all fixed and optimized hyperparameters\n","    hyperparameters = hp_dict\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        overview_df[\"hyperparameters\"] = overview_df[\"hyperparameters\"].astype(str)\n","        # save new df\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]},{"cell_type":"markdown","metadata":{"id":"zgMnvloo-cHL"},"source":["## MLP (feature selection)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1fAWsIth-cKd"},"outputs":[],"source":["def mlp_hyper_opt_fs(args):\n","    model = MLPClassifier(**args, max_iter=max_iter_mlp, random_state=0, n_iter_no_change=patience_mlp)\n","    model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","    val_preds = model.predict_proba(data_va_x_fs)\n","    loss = 1 - roc_auc_score(data_va_y, val_preds[:, 1])\n","    return {'loss': loss, 'status': STATUS_OK}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20593980,"status":"ok","timestamp":1659033308089,"user":{"displayName":"Jacob U.","userId":"11469197381579269429"},"user_tz":-120},"id":"s7P2z-3Hge4R","outputId":"b7223fa1-9b54-4f9d-f25d-f5246e8d1951"},"outputs":[{"name":"stdout","output_type":"stream","text":["performing training and optimization\n","starting hyperparameter optimization\n","  2%|▏         | 1/50 [01:32<1:15:47, 92.80s/it, best loss: 0.2931210807368215]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r  4%|▍         | 2/50 [03:32<1:26:39, 108.33s/it, best loss: 0.2931210807368215]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 10%|█         | 5/50 [10:51<1:37:53, 130.52s/it, best loss: 0.25475210660395853]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["\r 12%|█▏        | 6/50 [24:18<4:24:31, 360.72s/it, best loss: 0.25475210660395853]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 16%|█▌        | 8/50 [54:10<6:45:31, 579.32s/it, best loss: 0.23420965608465616]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 28%|██▊       | 14/50 [3:37:35<9:12:14, 920.41s/it, best loss: 0.23420965608465616]  "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 58%|█████▊    | 29/50 [4:22:45<31:26, 89.81s/it, best loss: 0.21357075494806976]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 64%|██████▍   | 32/50 [4:26:23<24:56, 83.11s/it, best loss: 0.18814759700176364]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 70%|███████   | 35/50 [4:32:47<27:11, 108.78s/it, best loss: 0.18814759700176364]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 80%|████████  | 40/50 [5:53:34<2:46:25, 998.56s/it, best loss: 0.18814759700176364] "]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 84%|████████▍ | 42/50 [6:24:19<2:17:51, 1033.91s/it, best loss: 0.18814759700176364]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 90%|█████████ | 45/50 [6:30:12<36:22, 436.54s/it, best loss: 0.18814759700176364]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n","/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":[" 98%|█████████▊| 49/50 [7:57:47<11:37, 697.71s/it, best loss: 0.18814759700176364]"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n","  ConvergenceWarning,\n","\n"]},{"name":"stdout","output_type":"stream","text":["100%|██████████| 50/50 [8:15:56<00:00, 595.14s/it, best loss: 0.18814759700176364]\n","the best MLP hyperparameters are: learning_rate_init 0.03162277660168379 | l2 0.004791443737261613 | hidden_layer_sizes (100, 100)\n","performing repetitions on different seeds\n","model information added to experimentation overview\n","|    | model_type   | filename                   | GPU_accelerator      | RAM      | data_features                | hyperparameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | train_performance_ROC-AUC_avg/std/max   | valid_performance_ROC-AUC_avg/std/max   | test_performance_ROC-AUC_avg/std/max   |\n","|---:|:-------------|:---------------------------|:---------------------|:---------|:-----------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------|:----------------------------------------|:---------------------------------------|\n","|  0 | SVM          | svm_opt_full_descr_5k_50   | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'C': 40.52181430939162, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.003877592396436093, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 950769, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [9.9155e-01 1.0000e-05 9.9157e-01]      | [7.9398e-01 7.0000e-05 7.9412e-01]      | [6.5325e-01 4.0000e-05 6.5331e-01]     |\n","|  1 | SVM          | svm_opt_feat_select_15k_50 | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'C': 0.9146720748918769, 'break_ties': False, 'cache_size': 1000, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.07830765964249707, 'kernel': 'rbf', 'max_iter': 5000, 'probability': True, 'random_state': 761449, 'shrinking': True, 'tol': 0.001, 'verbose': True}                                                                                                                                                                                                                         | [0.99813 0.      0.99813]               | [7.9447e-01 2.0000e-04 7.9488e-01]      | [7.5988e-01 2.5000e-04 7.6021e-01]     |\n","|  2 | RF           | rf_opt_full_descr_50       | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 13, 'max_features': 0.4, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0007866053743963098, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 606180, 'verbose': 0, 'warm_start': False}                                                                                                                         | [9.9116e-01 2.3000e-04 9.9144e-01]      | [0.78926 0.00851 0.80319]               | [0.78592 0.00485 0.79239]              |\n","|  3 | RF           | rf_opt_feat_select_50      | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0003677058219403621, 'min_samples_leaf': 3, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 10, 'n_jobs': -1, 'oob_score': False, 'random_state': 476257, 'verbose': 0, 'warm_start': False}                                                                                                                       | [0.95785 0.00362 0.96235]               | [0.74416 0.04167 0.81029]               | [0.73382 0.01442 0.76491]              |\n","|  4 | XGB          | xgb_opt_full_descr_50      | Tesla P100-PCIE-16GB | 27.33 GB | all features (233 feats)     | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.9436828391115852, 'gamma': 0.22369342452726626, 'learning_rate': 0.20941648886207803, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 826976, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.910169049816008, 'verbosity': 1} | [9.9891e-01 8.0000e-05 9.9908e-01]      | [0.79425 0.00737 0.80282]               | [0.75037 0.00928 0.76298]              |\n","|  5 | XGB          | xgb_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7144328026415806, 'gamma': 0.1982639800431708, 'learning_rate': 0.1246187700498615, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 4, 'missing': None, 'n_estimators': 300, 'n_jobs': -1, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 881799, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 27.501039505004883, 'seed': None, 'silent': None, 'subsample': 0.7215172738554442, 'verbosity': 1}  | [0.89512 0.0249  0.92884]               | [0.7885  0.00918 0.80097]               | [0.76791 0.01247 0.77909]              |\n","|  6 | MLP          | mlp_opt_full_descr_50      | Tesla T4             | 27.33 GB | all features (233 feats)     | {'activation': 'relu', 'alpha': 0.0002746800923919801, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.0031622776601683794, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 353006, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}       | [9.9970e-01 3.5000e-04 9.9998e-01]      | [0.7626  0.01725 0.79007]               | [0.70937 0.01347 0.73482]              |\n","|  7 | MLP          | mlp_opt_feat_select_50     | Tesla P100-PCIE-16GB | 27.33 GB | feature selection (67 feats) | {'activation': 'relu', 'alpha': 0.004791443737261613, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 100), 'learning_rate': 'constant', 'learning_rate_init': 0.03162277660168379, 'max_fun': 15000, 'max_iter': 300, 'momentum': 0.9, 'n_iter_no_change': 50, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 926051, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}               | [0.88876, 0.01725, 0.91386]             | [0.7595, 0.03619, 0.81185]              | [0.70339, 0.0151, 0.71833]             |\n"]}],"source":["filename_mlp_full_descr = \"mlp_opt_feat_select_50\"\n","\n","if file_exists(filename_mlp_full_descr+\".sav\") and file_exists(filename_mlp_full_descr+\"_performance\"):\n","    print(\"no training and optimization needed, everything can be loaded\")\n","\n","    # model\n","    print('\\n')\n","    print(\"best MLP model on feature selection is:\")\n","    loaded_model = pickle.load(open(filename_mlp_full_descr+\".sav\", 'rb'))\n","    print(loaded_model)\n","\n","    # performance\n","    perf_df = pd.read_parquet(filename_mlp_full_descr + \"_performance\")\n","    print('\\n')\n","    print(f\"mean ROC-AUC across {repetitions} different seeds\")\n","    print(f\"train: {round(np.average(perf_df['auc_roc'][0]), 5)} | validation: {round(np.average(perf_df['auc_roc'][1]), 5)}, test: {round(np.average(perf_df['auc_roc'][2]), 5)}\")\n","\n","    # overview\n","    print('\\n')\n","    print(\"experimentation overview:\")\n","    # load \n","    overview_df = pd.read_parquet(overview_df_filename)\n","    print(overview_df.to_markdown())\n","\n","else:\n","    print(\"performing training and optimization\")\n","\n","    # hyperparameter optimization\n","    print(\"starting hyperparameter optimization\")\n","    trials = Trials()\n","    best_results_mlp_feat_select = fmin(mlp_hyper_opt_fs, mlp_hyper_space, algo=tpe.suggest, max_evals=OPT_ITERS, trials=trials)\n","    \n","    text = (\n","            \"the best MLP hyperparameters are: \"\n","            f\"learning_rate_init {learning_rate_init_ls[best_results_mlp_feat_select['learning_rate_init']]} | \"\n","            f\"l2 {best_results_mlp_feat_select['alpha']} | \"\n","            f\"hidden_layer_sizes {hidden_layer_sizes_ls[best_results_mlp_feat_select['hidden_layer_sizes']]}\"\n","            )\n","    print(text)\n","    best_model = MLPClassifier(learning_rate_init= learning_rate_init_ls[best_results_mlp_feat_select['learning_rate_init']],\n","                               alpha=best_results_mlp_feat_select['alpha'],\n","                               hidden_layer_sizes = hidden_layer_sizes_ls[best_results_mlp_feat_select['hidden_layer_sizes']],\n","                               max_iter=max_iter_mlp, random_state=0, n_iter_no_change=patience_mlp)\n","\n","    # best_model.fit(data_tr_x.to_numpy().astype(float), data_tr_y.values.ravel().astype(int))\n","    best_model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","\n","    # save hyperparameters\n","    with open(filename_mlp_full_descr+\"_hps\", 'wb') as f:\n","        pickle.dump(best_results_mlp_feat_select, f)\n","    # loadable via ...\n","    # with open(filename_mlp_full_descr+\"_hps\", 'rb') as f:\n","    #    loaded_dict = pickle.load(f)\n","\n","    # save best model\n","    pickle.dump(best_model, open(filename_mlp_full_descr+\".sav\", 'wb'))\n","    # loadable via ...\n","    # best_model = pickle.load(open(filename_mlp_full_descr+\".sav\", 'rb'))\n","\n","    # repetitions for performance on different seeds\n","    tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc = [], [], [], [], [], [], [], [], [], []\n","    te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc = [], [], [], [], [], [], [], [], [], []\n","\n","    tr_lst = [tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc]\n","    va_lst = [va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc]\n","    te_lst = [te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]\n","\n","    print(\"performing repetitions on different seeds\")\n","    for i in range(repetitions):\n","        \n","        # first replicate model with initial seed\n","        if i == 0:\n","            seed = 0\n","        else:\n","            seed = np.random.randint(1, 999999) # all but initial random seed of 0\n","        \n","        best_model = MLPClassifier(learning_rate_init= learning_rate_init_ls[best_results_mlp_feat_select['learning_rate_init']],\n","                               alpha=best_results_mlp_feat_select['alpha'],\n","                               hidden_layer_sizes = hidden_layer_sizes_ls[best_results_mlp_feat_select['hidden_layer_sizes']],\n","                               max_iter=max_iter_mlp, random_state=seed, n_iter_no_change=patience_mlp)\n","        \n","        best_model.fit(data_tr_x_fs, data_tr_y.values.ravel())\n","\n","        # training metrics calc\n","        tr_pred = best_model.predict_proba(data_tr_x_fs)\n","        tr_metrics = list(statistical(data_tr_y, np.argmax(tr_pred, axis=1), tr_pred[:, 1]))\n","\n","        # validation metric calc\n","        va_pred = best_model.predict_proba(data_va_x_fs)\n","        va_metrics = list(statistical(data_va_y, np.argmax(va_pred, axis=1), va_pred[:, 1]))\n","\n","        # test metric calc\n","        te_pred = best_model.predict_proba(data_te_x_fs)\n","        te_metrics = list(statistical(data_te_y, np.argmax(te_pred, axis=1), te_pred[:, 1]))\n","\n","        # creating dataframe\n","        for j in range(len(tr_lst)):               \n","            tr_lst[j].append(tr_metrics[j])\n","            va_lst[j].append(va_metrics[j])\n","            te_lst[j].append(te_metrics[j])\n","\n","    metric_cls = [\"tn\", \"fp\", \"fn\", \"tp\", \"se\", \"sp\", \"acc\", \"mcc\", \"auc_prc\", \"auc_roc\"] \n","    metrics_data = [[\"train\", tr_tns, tr_fps, tr_fns, tr_tp, tr_se, tr_sp, tr_acc, tr_mcc, tr_auc_prc, tr_auc_roc],\n","                    [\"validation\", va_tns, va_fps, va_fns, va_tp, va_se, va_sp, va_acc, va_mcc, va_auc_prc, va_auc_roc],\n","                    [\"test\", te_tns, te_fps, te_fns, te_tp, te_se, te_sp, te_acc, te_mcc, te_auc_prc, te_auc_roc]]\n","    mlp_feat_select_perf = pd.DataFrame(metrics_data, columns = [\"split\"] + metric_cls)\n","    \n","    # save performance df\n","    mlp_feat_select_perf.to_parquet(filename_mlp_full_descr + \"_performance\", index=0)      \n","    # loadable via ...\n","    # mlp_feat_select_perf = pd.read_parquet(filename_mlp_full_descr + \"_performance\")\n","\n","    # add model info \n","    cols = [\"avg_auc_roc\", \"std_auc_roc\", \"top_roc_auc\"]\n","    tr_aggr = []\n","    va_aggr = []\n","    te_aggr = []\n","    results = [tr_aggr, va_aggr, te_aggr]\n","\n","    for i in range(len(results)):\n","        # avg_auc_roc\n","        results[i].append(round(np.average(mlp_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # std_auc_roc\n","        results[i].append(round(np.std(mlp_feat_select_perf[\"auc_roc\"][i]), 5))\n","        # top_roc_auc\n","        results[i].append(round(np.max(mlp_feat_select_perf[\"auc_roc\"][i]), 5))\n","\n","    cls = [\"model_type\", \"filename\", \"GPU_accelerator\", \"RAM\", \"data_features\", \"hyperparameters\", \"train_performance_ROC-AUC_avg/std/max\", \"valid_performance_ROC-AUC_avg/std/max\", \"test_performance_ROC-AUC_avg/std/max\"]\n","    model_type = \"MLP\"\n","    data_features = f\"feature selection ({data_tr_x_fs.shape[1]} feats)\"\n","    filename = filename_mlp_full_descr\n","    GPU_info = !nvidia-smi -L\n","    GPU_accelerator = re.search(r\"\\: (.*?)\\(\", str(GPU_info)).group(1)\n","    RAM = f\"{round(virtual_memory().total / 1e9, 2)} GB\"\n","    hp_dict = best_model.get_params() # all fixed and optimized hyperparameters\n","    hyperparameters = hp_dict\n","    tr_performance = tr_aggr\n","    va_performance = va_aggr\n","    te_performance = te_aggr\n","\n","    # does overview table exist?\n","    if not file_exists(overview_df_filename):\n","        # create dataframe with model info\n","        info = [[model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]]\n","        df = pd.DataFrame(info, columns=cls)\n","        # save\n","        df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(df.to_markdown())\n","    else:\n","        # load \n","        overview_df = pd.read_parquet(overview_df_filename)\n","\n","        # add row for model\n","        new_row = {}\n","        keys = cls\n","        values = [model_type, filename, GPU_accelerator, RAM, data_features, hyperparameters, tr_performance, va_performance, te_performance]\n","        for key in keys:\n","            for value in values:\n","                new_row[key] = value\n","                values.remove(value)\n","                break \n","        overview_df = overview_df.append(new_row, ignore_index=True)\n","        overview_df[\"hyperparameters\"] = overview_df[\"hyperparameters\"].astype(str)\n","        # save new df\n","        overview_df.to_parquet(overview_df_filename, index=0)\n","        print(\"model information added to experimentation overview\")\n","        print(overview_df.to_markdown())"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNyGuF16deHNNImueI3HXRx","collapsed_sections":["L_AM_YNN_a_H"],"machine_shape":"hm","name":"0_train_baselines.ipynb","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.7.13 ('GNN_thesis')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.13"},"vscode":{"interpreter":{"hash":"05a6106fea09248b97928b45a8f993ed9eda1dddec2745a1933e0c85e248786f"}}},"nbformat":4,"nbformat_minor":0}
